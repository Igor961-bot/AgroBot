{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9b0e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()              # uruchamia garbage collector\n",
    "torch.cuda.empty_cache()   # zwalnia pamięć zaalokowaną w cache\n",
    "torch.cuda.ipc_collect()   # dodatkowe czyszczenie pamięci współdzielonej\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b257ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"KRUS-debug\"\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")\n",
    "\n",
    "\n",
    "sorDEBUG = True\n",
    "DEBUG = sorDEBUG\n",
    "RETURN_STRING_WHEN_DEBUG_FALSE = True\n",
    "\n",
    "\n",
    "DATA_PATH = \"C:/Users/admin/Desktop/krus-chatbot/AgroBot/data/ustawa_with_paragraph_headers.md\"\n",
    "PERSIST_PATH = \"chroma_ustawa\"\n",
    "\n",
    "EMBEDDER_MODEL   = \"intfloat/multilingual-e5-base\"\n",
    "RERANKER_MODEL   = \"radlab/polish-cross-encoder\" \n",
    "BASE_MODEL_ID    = \"speakleash/Bielik-11B-v2.6-Instruct\"\n",
    "#\"CYFRAGOVPL/PLLuM-12B-chat\"\n",
    "\n",
    "RERANK_THRESHOLD = 0.30\n",
    "K_SIM   = 10\n",
    "K_FINAL = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fd8dd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0.dev20250903+cu128 | CUDA available: True | GPUs: 2\n",
      "Platform: Windows-11-10.0.26100-SP0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434cbccf7d214aa795becca4bff32b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model załadowany bez kwantyzacji\n"
     ]
    }
   ],
   "source": [
    "import os, torch, platform\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"   \n",
    "torch.backends.cuda.matmul.allow_tf32 = True    \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available(), \"| GPUs:\", torch.cuda.device_count())\n",
    "print(\"Platform:\", platform.platform()) \n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=True, trust_remote_code=False)\n",
    "except Exception as e:\n",
    "    print(\"Fast tokenizer fail → slow:\", e)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=False)\n",
    "\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,      \n",
    "    device_map=\"cuda:0\",               \n",
    "    low_cpu_mem_usage=False,\n",
    "    attn_implementation=\"sdpa\",      \n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "\n",
    "\n",
    "print(\"model załadowany bez kwantyzacji\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a4ec07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INDEX] Liczba dokumentów (ustępów): 444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9440\\2375576860.py:49: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceBgeEmbeddings(\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9440\\2375576860.py:62: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "import shutil, re\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "persist_path = PERSIST_PATH\n",
    "shutil.rmtree(persist_path, ignore_errors=True)\n",
    "os.makedirs(persist_path, exist_ok=True)\n",
    "\n",
    "docs_raw = TextLoader(DATA_PATH, encoding=\"utf-8\").load()\n",
    "header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"###\", \"ustep\")])\n",
    "chunks = header_splitter.split_text(docs_raw[0].page_content)\n",
    "\n",
    "# meta: <!-- chapter:1 article:16a paragraph:3 id:ch1-art16a-ust3 -->\n",
    "meta_re = re.compile(\n",
    "    r\"<!--\\s*chapter\\s*:\\s*(\\d+)\\s+article\\s*:\\s*([0-9a-z]+)\\s+paragraph\\s*:\\s*([0-9a-z]+)\\s+id\\s*:\\s*([^\\s>]+)\\s*-->\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "normed: List[Document] = []\n",
    "for d in chunks:\n",
    "    md = dict(d.metadata)\n",
    "    header_text = md.get(\"ustep\", \"\") or \"\"\n",
    "    source_for_meta = header_text + \"\\n\" + d.page_content\n",
    "    m = meta_re.search(source_for_meta)\n",
    "    if m:\n",
    "        md[\"chapter\"]   = int(m.group(1))\n",
    "        md[\"article\"]   = m.group(2).lower()\n",
    "        md[\"paragraph\"] = m.group(3).lower()\n",
    "        md[\"id\"]        = m.group(4)\n",
    "        md[\"rozdzial\"]  = md[\"chapter\"]\n",
    "        md[\"artykul\"]   = md[\"article\"]\n",
    "        md[\"ust\"]       = md[\"paragraph\"]\n",
    "\n",
    "    clean_header = meta_re.sub(\"\", header_text).strip()\n",
    "    if clean_header:\n",
    "        md[\"ustep\"] = clean_header\n",
    "\n",
    "    content = meta_re.sub(\"\", d.page_content).strip()\n",
    "    normed.append(Document(page_content=content, metadata=md))\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"[INDEX] Liczba dokumentów (ustępów): {len(normed)}\")\n",
    "\n",
    "embedder = HuggingFaceBgeEmbeddings(\n",
    "    model_name=EMBEDDER_MODEL,\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "shutil.rmtree(persist_path, ignore_errors=True)\n",
    "db = Chroma.from_documents(\n",
    "    documents=normed,\n",
    "    embedding=embedder,\n",
    "    persist_directory=persist_path,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "db.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8b3291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\onnx\\_internal\\registration.py:162: OnnxExporterWarning: Symbolic function 'aten::scaled_dot_product_attention' already registered for opset 14. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:196: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9440\\3833670433.py:172: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Cross Encoder załadowany na CPU!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9440\\3833670433.py:193: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=hf_pipe)\n"
     ]
    }
   ],
   "source": [
    "K_SIM = globals().get(\"K_SIM\", 12)\n",
    "K_FINAL = globals().get(\"K_FINAL\", 6)\n",
    "RERANK_THRESHOLD = globals().get(\"RERANK_THRESHOLD\", None) \n",
    "RERANKER_MODEL = globals().get(\"RERANKER_MODEL\", \"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "import os, re, shutil, unicodedata, asyncio\n",
    "from typing import List, Optional, Dict, Any, Tuple, Callable\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.callbacks import CallbackManager\n",
    "from langchain.callbacks.tracers.langchain import LangChainTracer\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.documents import Document  \n",
    "\n",
    "# Reranker + LLM\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import pipeline as hf_pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import onnxruntime as ort\n",
    "\n",
    "def strip_accents_lower(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    return s.lower().strip()\n",
    "\n",
    "# --- wymagane globalne uchwyty (jak w Twojej komórce) ---\n",
    "if \"db\" not in globals():\n",
    "    raise RuntimeError(\"Brak globalnej bazy `db` (Chroma). Zainicjalizuj ją przed załadowaniem skryptu.\")\n",
    "model = gen_model\n",
    "\n",
    "ROLE_CUT_RE = re.compile(\n",
    "    r\"(?i)\"                              \n",
    "    r\"(###\\s*(?:user|asystent|dokumenty|system)?\\s*:?\" \n",
    "    r\"|(?:^|\\s)(?:user|asystent|dokumenty|system)\\s*:\" \n",
    "    r\"|<\\s*(?:user|assistant|docs?|system)\\s*>)\"      \n",
    ")\n",
    "\n",
    "def cut_after_role_markers(s: str) -> str:\n",
    "    if not s:\n",
    "        return s\n",
    "    m = ROLE_CUT_RE.search(s)\n",
    "    return s[:m.start()].rstrip() if m else s\n",
    "\n",
    "if globals().get(\"model\", None) is None or globals().get(\"tokenizer\", None) is None:\n",
    "    raise RuntimeError(\"Załaduj wcześniej LLM do zmiennych `model` i `tokenizer`.\")\n",
    "\n",
    "# =======================\n",
    "# RERANKER: ONNX na CPU\n",
    "# =======================\n",
    "device = \"cpu\"\n",
    "\n",
    "class OptimizedONNXCrossEncoder:\n",
    "    def __init__(self, model_name, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        providers = [\"CPUExecutionProvider\"]\n",
    "        try:\n",
    "            self.model = ORTModelForSequenceClassification.from_pretrained(\n",
    "                model_name,\n",
    "                provider=providers[0],\n",
    "                export=True\n",
    "            )\n",
    "            print(\"ONNX Cross Encoder załadowany na CPU!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fallback do zwykłego CrossEncoder na CPU: {e}\")\n",
    "            from sentence_transformers import CrossEncoder\n",
    "            self.fallback_model = CrossEncoder(model_name, device=\"cpu\")\n",
    "            self.model = None\n",
    "    \n",
    "    def predict(self, pairs, batch_size=32, **kwargs):\n",
    "        if self.model is None:\n",
    "            return self.fallback_model.predict(pairs, batch_size=batch_size)\n",
    "        \n",
    "        all_scores = []\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = pairs[i:i+batch_size]\n",
    "            texts_1 = [p[0] for p in batch]\n",
    "            texts_2 = [p[1] for p in batch]\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                texts_1, texts_2,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                if outputs.logits.shape[-1] == 2:\n",
    "                    scores = torch.softmax(outputs.logits, dim=-1)[:, 1]\n",
    "                else:\n",
    "                    scores = outputs.logits[:, 0]\n",
    "                scores = scores.cpu().numpy()\n",
    "                all_scores.extend(scores.tolist())\n",
    "        return np.array(all_scores)\n",
    "\n",
    "cross_encoder = OptimizedONNXCrossEncoder(RERANKER_MODEL, device=device)\n",
    "\n",
    "# =======================\n",
    "# Parsowanie referencji\n",
    "# =======================\n",
    "REF_RE_EXT = re.compile(\n",
    "    r\"(?:art\\.?\\s*(?P<art>[0-9]+[a-z]?))\"\n",
    "    r\"(?:\\s*(?:ust(?:\\.|ęp)?|ustep)\\s*(?P<ust>[0-9]+[a-z]?))?\"\n",
    "    r\"(?:\\s*(?:pkt\\.?)\\s*(?P<pkt>[0-9]+[a-z]?))?\"\n",
    "    r\"(?:\\s*(?:lit\\.?)\\s*(?P<lit>[a-z]))?\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "def parse_ref_ext(query: str) -> Optional[Dict[str, str]]:\n",
    "    m = REF_RE_EXT.search(query or \"\")\n",
    "    if not m:\n",
    "        return None\n",
    "    ref = {}\n",
    "    if m.group(\"art\"): ref[\"article\"]   = m.group(\"art\").lower()\n",
    "    if m.group(\"ust\"): ref[\"paragraph\"] = m.group(\"ust\").lower()\n",
    "    if m.group(\"pkt\"): ref[\"punkt\"]     = m.group(\"pkt\").lower()\n",
    "    if m.group(\"lit\"): ref[\"litera\"]    = m.group(\"lit\").lower()\n",
    "    return ref if ref else None\n",
    "\n",
    "# =======================\n",
    "# Retriever + reranker\n",
    "# =======================\n",
    "def retrieve_basic(query: str, k_sim: int = K_SIM, k_final: int = K_FINAL, rerank_threshold: float | None = RERANK_THRESHOLD) -> List[Document]:\n",
    "    ref = parse_ref_ext(query)\n",
    "    filter_dict = {}\n",
    "    if ref:\n",
    "        if \"article\" in ref:   filter_dict[\"article\"]   = ref[\"article\"]\n",
    "        if \"paragraph\" in ref: filter_dict[\"paragraph\"] = ref[\"paragraph\"]\n",
    "    if not filter_dict:\n",
    "        filter_dict = None\n",
    "\n",
    "    docs = db.similarity_search(query, k=k_sim, filter=filter_dict)\n",
    "    if not docs:\n",
    "        return []\n",
    "\n",
    "    pairs = [(query, d.page_content) for d in docs]\n",
    "    scores = cross_encoder.predict(pairs, batch_size=32) \n",
    "    scores = np.asarray(scores, dtype=float)\n",
    "\n",
    "    scored_docs = [(d, s) for d, s in zip(docs, scores)]\n",
    "    if rerank_threshold is not None:\n",
    "        scored_docs = [(d, s) for d, s in scored_docs if s >= rerank_threshold]\n",
    "        if not scored_docs:\n",
    "            return []\n",
    "\n",
    "    scored_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)[:k_final]\n",
    "    out: List[Document] = []\n",
    "    for d, s in scored_docs:\n",
    "        md = dict(d.metadata or {})\n",
    "        md[\"rerank_score\"] = float(s)\n",
    "        d.metadata = md\n",
    "        out.append(d)\n",
    "    return out\n",
    "\n",
    "# =======================\n",
    "# Memory + LLM\n",
    "# =======================\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=3, memory_key=\"chat_history\", return_messages=True, output_key=\"answer\"\n",
    ")\n",
    "\n",
    "hf_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=globals().get(\"model\"),\n",
    "    tokenizer=globals().get(\"tokenizer\"),\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.35,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.0,\n",
    "    pad_token_id=(getattr(globals().get(\"tokenizer\"), \"eos_token_id\", None)),\n",
    "    eos_token_id=(getattr(globals().get(\"tokenizer\"), \"eos_token_id\", None)),\n",
    "    return_full_text=False,\n",
    "    use_cache=True,\n",
    "    batch_size=1,  \n",
    "    num_beams=1,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipe)\n",
    "\n",
    "# Dwa prompty (opcjonalnie różne)\n",
    "prompt_base = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"### System:\\n\"\n",
    "        \"Jesteś ekspertem prawa ubezpieczeń społecznych rolników. \"\n",
    "        \"Odpowiadasz WYŁĄCZNIE na podstawie Dokumentów poniżej. \"\n",
    "        \"Twoim zadaniem jest odpowiedzenie na pytanie na podstawie Dokumentów. \"\n",
    "        \"Jeśli w dokumentach jest BRAK, powiedz, że nie masz wiedzy na ten temat.\\n\"\n",
    "        \"Formatowanie: bez **…** ani __…__. Limit 6–8 zdań lub 8 punktów.\\n\"\n",
    "        \"### User:\\n{question}\\n\\n\"\n",
    "        \"### Dokumenty:\\n{context}\\n\"\n",
    "        \"### Asystent:\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt_followup = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"### System:\\n\"\n",
    "        \"Kontynuacja rozmowy. Odpowiadaj WYŁĄCZNIE w oparciu o poniższe dokumenty, \"\n",
    "        \"które akumulujemy w toku dopytań użytkownika. \"\n",
    "        \"Jeśli dokumenty nie zawierają odpowiedzi, powiedz o tym wprost.\\n\"\n",
    "        \"Format: bez **…** ani __…__. Zwięźle: 5–7 zdań lub 6 punktów.\\n\"\n",
    "        \"### User:\\n{question}\\n\\n\"\n",
    "        \"### Dokumenty (skumulowane):\\n{context}\\n\"\n",
    "        \"### Asystent:\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "tracer = LangChainTracer()\n",
    "callback_manager = CallbackManager([tracer])\n",
    "\n",
    "class FunctionRetriever(BaseRetriever):\n",
    "    k_sim: int\n",
    "    k_final: int\n",
    "    rerank_threshold: Optional[float] = None\n",
    "    _fn: Callable[..., List[Document]] = PrivateAttr()\n",
    "\n",
    "    def __init__(self, fn: Callable[..., List[Document]], **data):\n",
    "        super().__init__(**data)\n",
    "        self._fn = fn\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self._fn(query, k_sim=self.k_sim, k_final=self.k_final, rerank_threshold=self.rerank_threshold)\n",
    "\n",
    "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(None, lambda: self._get_relevant_documents(query))\n",
    "\n",
    "init_retriever = FunctionRetriever(fn=retrieve_basic, k_sim=K_SIM, k_final=K_FINAL, rerank_threshold=RERANK_THRESHOLD)\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=init_retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_base},\n",
    "    return_source_documents=True,\n",
    "    output_key=\"answer\",\n",
    "    callback_manager=callback_manager\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# Formatowanie odpowiedzi\n",
    "# =======================\n",
    "def _build_citations_block(docs: List[Document]) -> str:\n",
    "    if not docs:\n",
    "        return \"Cytowane ustępy:\\n(brak)\\n\"\n",
    "    lines = [\"Cytowane ustępy:\"]\n",
    "    for d in docs:\n",
    "        md = d.metadata or {}\n",
    "        rozdz = md.get(\"rozdzial\", md.get(\"chapter\"))\n",
    "        art   = md.get(\"artykul\",  md.get(\"article\"))\n",
    "        ust   = md.get(\"ust\",      md.get(\"paragraph\"))\n",
    "        pid   = md.get(\"id\", f\"ch{rozdz}-art{art}-ust{ust}\")\n",
    "        lines.append(f\"- [{pid}] Rozdz.{rozdz} Art.{art} Ust.{ust}\")\n",
    "    return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "def format_docs_for_prompt(docs: List[Document]) -> str:\n",
    "    blocks = []\n",
    "    for d in docs:\n",
    "        md = d.metadata or {}\n",
    "        rozdz = md.get(\"rozdzial\", md.get(\"chapter\"))\n",
    "        art   = md.get(\"artykul\",  md.get(\"article\"))\n",
    "        ust   = md.get(\"ust\",      md.get(\"paragraph\"))\n",
    "        pid   = md.get(\"id\", f\"ch{rozdz}-art{art}-ust{ust}\")\n",
    "        blocks.append(f\"[{pid}]\\n{d.page_content}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "def log_rerank_scores(docs: List[Document], header: str = \"DEBUG rerank scores\") -> None:\n",
    "    if not DEBUG: return\n",
    "    print(header)\n",
    "    if not docs:\n",
    "        print(\"(brak dokumentów)\"); return\n",
    "    for d in docs:\n",
    "        md = d.metadata or {}\n",
    "        pid = md.get(\"id\") or f\"ch{md.get('chapter')}-art{md.get('article')}-ust{md.get('paragraph')}\"\n",
    "        sc  = md.get(\"rerank_score\")\n",
    "        print(f\"- {pid}: score={sc:.6f}\" if isinstance(sc, (int, float)) else f\"- {pid}: score=(brak)\")\n",
    "\n",
    "def strip_markdown_bold(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    text = re.sub(r\"\\*\\*(.*?)\\*\\*\", r\"\\1\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"__(.*?)__\", r\"\\1\", text, flags=re.DOTALL)\n",
    "    text = text.replace(\"**\", \"\").replace(\"__\", \"\")\n",
    "    return text\n",
    "\n",
    "_SENT_ENDERS = \".?!…\"\n",
    "_CLOSERS = \"”»\\\")]’\"\n",
    "\n",
    "def _rstrip_u(s: str) -> str:\n",
    "    return s.rstrip(\" \\t\\r\\n\\u00A0\")\n",
    "\n",
    "_LIST_MARKER_ONLY_RE = re.compile(\n",
    "    r\"\"\"^[\\s\\u00A0]*(\n",
    "            [-*+•·—–]\n",
    "          | (?:\\(?\\d+[a-z]?\\)|\\d+[a-z]?[.)])\n",
    "          | (?:\\(?[ivxlcdm]+\\)|[ivxlcdm]+[.)])\n",
    "          | (?:[a-z][.)])\n",
    "        )[\\s\\u00A0]*$\"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE\n",
    ")\n",
    "def _strip_trailing_empty_list_item(s: str) -> tuple[str, bool]:\n",
    "    if not s:\n",
    "        return s, False\n",
    "    s2 = _rstrip_u(s)\n",
    "    if not s2:\n",
    "        return s2, (s2 != s)\n",
    "    lines = s2.splitlines()\n",
    "    last = _rstrip_u(lines[-1])\n",
    "    if _LIST_MARKER_ONLY_RE.match(last or \"\"):\n",
    "        return _rstrip_u(\"\\n\".join(lines[:-1])), True\n",
    "    return s2, False\n",
    "\n",
    "def _ends_with_full_stop(s: str) -> bool:\n",
    "    return re.search(rf\"[{re.escape(_SENT_ENDERS)}][{re.escape(_CLOSERS)}]*[\\s\\u00A0]*$\", s) is not None\n",
    "\n",
    "_ABBR_SET = {\"art\",\"ust\",\"pkt\",\"lit\",\"tj\",\"tzw\",\"np\",\"itd\",\"itp\",\"m.in\",\"prof\",\"dr\",\"nr\",\"poz\",\"cd\",\"al\",\"ul\",\"pl\",\"św\",\"sw\"}\n",
    "def _last_token_before_dot_for_trim(buf: str) -> str:\n",
    "    m = re.search(r\"([A-Za-zÀ-ÖØ-öø-ÿŁŚŻŹĆŃÓÄÖÜĄĘłśżźćńóäöüąę]+)\\.\\s*$\", buf)\n",
    "    return (m.group(1).lower() if m else \"\")\n",
    "def _is_abbreviation_dot(text: str, dot_pos: int) -> bool:\n",
    "    prev = text[:dot_pos+1]\n",
    "    tok = _last_token_before_dot_for_trim(prev)\n",
    "    return tok in _ABBR_SET\n",
    "def _find_last_safe_boundary(s: str) -> int | None:\n",
    "    i = len(s) - 1\n",
    "    while i >= 0 and (s[i].isspace() or s[i] in _CLOSERS or s[i] == \"\\u00A0\"):\n",
    "        i -= 1\n",
    "    while i >= 0:\n",
    "        ch = s[i]\n",
    "        if ch in _SENT_ENDERS:\n",
    "            if ch == \".\" and _is_abbreviation_dot(s[:i+1], i):\n",
    "                i -= 1\n",
    "                continue\n",
    "            return i + 1\n",
    "        i -= 1\n",
    "    return None\n",
    "def trim_incomplete_sentences(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    text = cut_after_role_markers(text)\n",
    "    s = _rstrip_u(text)\n",
    "    changed = True\n",
    "    while changed:\n",
    "        s, changed = _strip_trailing_empty_list_item(s)\n",
    "    if s.endswith(\":\"):\n",
    "        last_nl = s.rfind(\"\\n\")\n",
    "        s = _rstrip_u(s[:last_nl]) if last_nl != -1 else \"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    if _ends_with_full_stop(s):\n",
    "        return s\n",
    "    cut = _find_last_safe_boundary(s)\n",
    "    if cut is None:\n",
    "        return s\n",
    "    return _rstrip_u(s[:cut])\n",
    "\n",
    "def _finalize_return(text: str, docs: List[Document], mode: str):\n",
    "    # dopisujemy zachętę do dopytania (zostaje jak było)\n",
    "    hint = \"\\n\\n(Jeśli chcesz dopytać, kliknij „Chciałbym dopytać”, a dodam kolejny ustęp do kontekstu.)\"\n",
    "    text_out = text + hint\n",
    "\n",
    "    debug = [{\"id\": (d.metadata or {}).get(\"id\"),\n",
    "              \"score\": (d.metadata or {}).get(\"rerank_score\")} for d in (docs or [])]\n",
    "    payload = {\"answer\": text_out, \"source_documents\": docs, \"debug\": {\"mode\": mode, \"rerank\": debug}}\n",
    "\n",
    "    # USUWAMY efekt uboczny printowania, zwracamy tylko payload.\n",
    "    # Jeśli chcesz zachować opcjonalne drukowanie z tego miejsca, dodaj\n",
    "    # flagę np. PRINT_FROM_CORE=True i owiń print w if.\n",
    "    # if DEBUG and PRINT_FROM_CORE:\n",
    "    #     print(\"\\nODPOWIEDŹ\\n\", text_out, \"\\n\\n *Mogę popełniać błędy...*\\n\")\n",
    "\n",
    "    return payload\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Smalltalk\n",
    "# =======================\n",
    "_SMALLTALK_RULES = [\n",
    "    (r\"^(czesc|cze|hej|heja|hejka|witam|siema|elo|halo|dzien dobry|dobry wieczor)\\b\",\n",
    "     \"Cześć! W czym mogę pomóc w sprawie KRUS/ustawy?\"),\n",
    "    (r\"\\b(dzieki|dziekuje|dzieki wielkie|dziekuje bardzo|thx|thanks)\\b\",\n",
    "     \"Nie ma sprawy! Jeśli chcesz, podaj kolejne pytanie.\"),\n",
    "]\n",
    "def smalltalk_reply(user_q: str) -> Optional[str]:\n",
    "    qn = strip_accents_lower(user_q)\n",
    "    for pat, resp in _SMALLTALK_RULES:\n",
    "        if re.search(pat, qn, flags=re.IGNORECASE):\n",
    "            return resp\n",
    "    return None\n",
    "\n",
    "# =======================\n",
    "# Stan rozmowy (manual follow-up)\n",
    "# =======================\n",
    "class ConversationState:\n",
    "    def __init__(self):\n",
    "        self.last_article_num: Optional[str] = None\n",
    "        self.last_paragraph_num: Optional[str] = None\n",
    "        self.last_docs: List[Document] = []\n",
    "        self.accum_docs: List[Document] = []   # AKUMULOWANY KONTEKST\n",
    "        self.last_query: Optional[str] = None\n",
    "\n",
    "STATE = ConversationState()\n",
    "_FOLLOW_UP_NEXT = False\n",
    "\n",
    "def want_follow_up() -> None:\n",
    "    \"\"\"Wywołaj PRZED kolejnym ask(), jeśli user kliknął 'Chciałbym dopytać'.\"\"\"\n",
    "    global _FOLLOW_UP_NEXT\n",
    "    _FOLLOW_UP_NEXT = True\n",
    "\n",
    "def reset_context() -> None:\n",
    "    \"\"\"Czyści akumulowany kontekst i metadane.\"\"\"\n",
    "    STATE.accum_docs.clear()\n",
    "    STATE.last_docs.clear()\n",
    "    STATE.last_article_num = None\n",
    "    STATE.last_paragraph_num = None\n",
    "    STATE.last_query = None\n",
    "\n",
    "def _short_doc_label(d: Document) -> str:\n",
    "    md = d.metadata or {}\n",
    "    rozdz = md.get(\"rozdzial\", md.get(\"chapter\"))\n",
    "    art   = md.get(\"artykul\",  md.get(\"article\"))\n",
    "    ust   = md.get(\"ust\",      md.get(\"paragraph\"))\n",
    "    pid   = md.get(\"id\", f\"ch{rozdz}-art{art}-ust{ust}\")\n",
    "    return f\"{pid}\"\n",
    "\n",
    "def _update_state_from_docs(docs: List[Document], user_q: str):\n",
    "    if not docs: return\n",
    "    STATE.last_docs = docs[:]\n",
    "    STATE.last_query = user_q\n",
    "    md0 = docs[0].metadata or {}\n",
    "    STATE.last_article_num   = (md0.get(\"artykul\")  or md0.get(\"article\"))\n",
    "    STATE.last_paragraph_num = (md0.get(\"ust\")      or md0.get(\"paragraph\"))\n",
    "\n",
    "# =======================\n",
    "# Router\n",
    "# =======================\n",
    "def route_query(query: str):\n",
    "    qn = strip_accents_lower(query)\n",
    "    ref = parse_ref_ext(query)\n",
    "    if DEBUG:\n",
    "        print(f\"[ROUTER] raw='{query}' | norm='{qn}' | ref={ref}\")\n",
    "    if ref:\n",
    "        return \"EXPLICIT_REF\", {\"ref\": ref}\n",
    "    return \"GENERAL\", {\"query\": query}\n",
    "\n",
    "# =======================\n",
    "# Generacja\n",
    "# =======================\n",
    "def _llm_generate(prompt_tmpl: PromptTemplate, **kwargs) -> str:\n",
    "    text = prompt_tmpl.format(**kwargs)\n",
    "    out = llm.invoke(text)\n",
    "    return (out or \"\").strip()\n",
    "\n",
    "def answer_from_docs(question: str, docs: List[Document], *, followup: bool):\n",
    "    ctx = format_docs_for_prompt(docs) if docs else \"(brak dokumentów)\"\n",
    "    p = prompt_followup if followup else prompt_base\n",
    "    answer = _llm_generate(p, question=question, context=ctx)\n",
    "    answer = trim_incomplete_sentences(strip_markdown_bold(answer)) or answer\n",
    "    final_text = f\"{_build_citations_block(docs)}\\nOdpowiedź:\\n{answer}\"\n",
    "    return _finalize_return(final_text, docs, mode=(\"follow_up\" if followup else \"new_query\"))\n",
    "\n",
    "# =======================\n",
    "# Główna funkcja\n",
    "# =======================\n",
    "def ask(q: str, reset_memory: bool=False):\n",
    "    if reset_memory:\n",
    "        try:\n",
    "            qa_chain.memory.clear()\n",
    "        except Exception:\n",
    "            pass\n",
    "        reset_context()\n",
    "\n",
    "    st = smalltalk_reply(q)\n",
    "    if st is not None:\n",
    "        return _finalize_return(st, [], mode=\"smalltalk\")\n",
    "\n",
    "    action, payload = route_query(q)\n",
    "    if action == \"EXPLICIT_REF\":\n",
    "        ref = payload[\"ref\"]\n",
    "        flt = {}\n",
    "        if \"article\" in ref:   flt[\"article\"]   = ref[\"article\"]\n",
    "        if \"paragraph\" in ref: flt[\"paragraph\"] = ref[\"paragraph\"]\n",
    "        docs = db.similarity_search(\"treść przepisu\", k=5, filter=flt)\n",
    "        if DEBUG:\n",
    "            print(\"[ROUTER] EXPLICIT_REF → filter\", flt, \"→ docs:\", [ (d.metadata or {}).get(\"id\") for d in docs ])\n",
    "        if docs:\n",
    "            _update_state_from_docs(docs, q)\n",
    "            content = strip_markdown_bold(docs[0].page_content or \"\")\n",
    "            content = trim_incomplete_sentences(content) or content\n",
    "            final_text = f\"{_build_citations_block(docs)}\\nOdpowiedź (pełny przepis):\\n{content}\"\n",
    "            return _finalize_return(final_text, docs, mode=\"explicit\")\n",
    "        else:\n",
    "            return _finalize_return(\"Nie znalazłem takiego artykułu/ustępu.\", [], mode=\"explicit\")\n",
    "\n",
    "    # ========= RĘCZNE DOPYTANIE =========\n",
    "    global _FOLLOW_UP_NEXT\n",
    "    if _FOLLOW_UP_NEXT:\n",
    "        _FOLLOW_UP_NEXT = False  # zużyj flagę\n",
    "\n",
    "        # spróbuj dociągnąć TOP-1 w obrębie ostatniego artykułu\n",
    "        docs_narrow: List[Document] = []\n",
    "        if STATE.last_article_num:\n",
    "            flt = {\"article\": STATE.last_article_num}\n",
    "            docs_tmp = db.similarity_search(q, k=K_SIM, filter=flt)\n",
    "            if docs_tmp:\n",
    "                pairs = [(q, d.page_content) for d in docs_tmp]\n",
    "                scores = cross_encoder.predict(pairs, batch_size=32)\n",
    "                scored = sorted([(d, float(s)) for d, s in zip(docs_tmp, np.asarray(scores))],\n",
    "                                key=lambda x: x[1], reverse=True)[:1]\n",
    "                docs_narrow = [d for d, _ in scored]\n",
    "\n",
    "        # fallback: global TOP-1\n",
    "        if not docs_narrow:\n",
    "            top_global = retrieve_basic(q, k_sim=K_SIM, k_final=1, rerank_threshold=RERANK_THRESHOLD)\n",
    "            docs_narrow = top_global[:1] if top_global else []\n",
    "\n",
    "        # dołóż do stosu (deduplikacja po id)\n",
    "        def _doc_id(d: Document) -> str:\n",
    "            md = d.metadata or {}\n",
    "            return md.get(\"id\") or f\"ch{md.get('chapter')}-art{md.get('article')}-ust{md.get('paragraph')}\"\n",
    "        seen = {_doc_id(d) for d in STATE.accum_docs}\n",
    "        for d in docs_narrow:\n",
    "            if _doc_id(d) not in seen:\n",
    "                STATE.accum_docs.append(d)\n",
    "\n",
    "        if docs_narrow:\n",
    "            _update_state_from_docs(docs_narrow, q)\n",
    "\n",
    "        context_docs = STATE.accum_docs if STATE.accum_docs else docs_narrow\n",
    "        return answer_from_docs(q, context_docs, followup=True)\n",
    "\n",
    "    # ========= NOWE PYTANIE =========\n",
    "    res = qa_chain.invoke({\"question\": q})\n",
    "    docs = res.get(\"source_documents\", []) or []\n",
    "    _update_state_from_docs(docs, q)\n",
    "\n",
    "    # zaczynamy stos od TOP-1 (Twoja koncepcja)\n",
    "    STATE.accum_docs = [docs[0]] if docs else []\n",
    "\n",
    "    # odpowiedź z aktualnego stosu (czyli TOP-1 przy pierwszym pytaniu)\n",
    "    return answer_from_docs(q, STATE.accum_docs if STATE.accum_docs else docs, followup=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daf3476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ROUTER] raw='kto moze byc ubezpieczony w KRUS' | norm='kto moze byc ubezpieczony w krus' | ref=None\n",
      "{'answer': 'Cytowane ustępy:\\n- [ch1-art2-ust3] Rozdz.1 Art.2 Ust.3\\n\\nOdpowiedź:\\nW KRUS mogą być ubezpieczeni rolnicy oraz ich domownicy, a także inne osoby pracujące w gospodarstwie rolnym, które spełniają określone warunki. Do ubezpieczenia w KRUS kwalifikują się m.in.:\\n\\n1. Rolnicy prowadzący działalność rolniczą na własny rachunek.\\n2. Domownicy rolnika, czyli osoby bliskie rolnikowi, które stale pracują w gospodarstwie rolnym.\\n3. Osoby, które zaprzestały prowadzenia działalności rolniczej, ale mają ustalone prawo do emerytury lub renty rolniczej.\\n4. Inne osoby pracujące w gospodarstwie rolnym, jeśli nie podlegają ubezpieczeniu w ZUS (np. studenci, uczniowie).\\n\\nWażne jest, aby osoby te prowadziły wyłącznie działalność rolniczą lub były z nią związane, a ich gospodarstwo spełniało określone kryteria obszarowe. Ubezpieczenie w KRUS obejmuje zarówno ubezpieczenie emerytalno-rentowe, jak i wypadkowe, chorobowe oraz macierzyńskie.\\n\\n(Jeśli chcesz dopytać, kliknij „Chciałbym dopytać”, a dodam kolejny ustęp do kontekstu.)', 'source_documents': [Document(metadata={'artykul': '2', 'ustep': 'Ust. 3', 'paragraph': '3', 'id': 'ch1-art2-ust3', 'article': '2', 'rozdzial': 1, 'ust': '3', 'chapter': 1, 'rerank_score': 0.6693549156188965}, page_content='Interesy ogółu ubezpieczonych iświadczeniobiorców, dotyczące ubezpieczenia idziałalności Kasy, reprezentuje Rada Ubezpieczenia Społecznego Rolników, zwana dalej „RadąRolników”.')], 'debug': {'mode': 'new_query', 'rerank': [{'id': 'ch1-art2-ust3', 'score': 0.6693549156188965}]}}\n"
     ]
    }
   ],
   "source": [
    "print(ask(\"kto moze byc ubezpieczony w KRUS\", reset_memory=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa040da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ROUTER] raw='co to znaczy osoba niezdolna do samodzielnej egzystencji?' | norm='co to znaczy osoba niezdolna do samodzielnej egzystencji?' | ref=None\n",
      "\n",
      "Cytowane ustępy:\n",
      "- [ch2-art7-ust4] Rozdz.2 Art.7 Ust.4\n",
      "\n",
      "Odpowiedź:\n",
      "Osoba niezdolna do samodzielnej egzystencji to emeryt lub rencista, który posiada orzeczenie stwierdzające, że nie jest w stanie samodzielnie zaspokajać podstawowych potrzeb życiowych, takich jak jedzenie, ubieranie się, higiena osobista czy poruszanie się. Takie orzeczenie zwalnia tę osobę z obowiązku przestrzegania określonych przepisów, które mogłyby dotyczyć innych emerytów i rencistów. \n",
      "\n",
      "1. Orzeczenie o niezdolności do samodzielnej egzystencji.\n",
      "2. Zwolnienie z przepisów ust. 2 i 3.\n",
      "3. Podstawowe potrzeby życiowe.\n",
      "4. Emeryci i renciści jako grupa objęta tym przepisem.\n",
      "5. Brak obowiązku przestrzegania określonych regulacji.\n",
      "6. Znaczenie orzeczenia w kontekście prawnym.\n",
      "7. Ograniczenia w samodzielnym funkcjonowaniu.\n",
      "8. Specyficzne wyłączenie z przepisów.\n",
      "\n",
      "(Jeśli chcesz dopytać, kliknij „Chciałbym dopytać”, a dodam kolejny ustęp do kontekstu.)\n",
      "\n",
      "\n",
      "\n",
      "(\n",
      "\n",
      "[ROUTER] raw='t' | norm='t' | ref=None\n",
      "\n",
      "Cytowane ustępy:\n",
      "- [ch2-art7-ust4] Rozdz.2 Art.7 Ust.4\n",
      "- [ch2-art7-ust1] Rozdz.2 Art.7 Ust.1\n",
      "\n",
      "Odpowiedź:\n",
      "Oto odpowiedź na Twoje pytanie:\n",
      "\n",
      "Emeryci i renciści, którzy mają orzeczoną niezdolność do samodzielnej egzystencji, nie podlegają obowiązkowemu ubezpieczeniu wypadkowemu, chorobowemu i macierzyńskiemu. Dotyczy to zarówno rolników, jak i domowników rolników, o ile spełniają warunki określone w art. 7 ust. 1 pkt 1 i 2 ustawy o ubezpieczeniu społecznym rolników. Wyjątek ten wynika z art. 7 ust. 4 tej ustawy. \n",
      "\n",
      "1. Emeryci i renciści z niezdolnością do samodzielnej egzystencji nie podlegają obowiązkowemu ubezpieczeniu.\n",
      "2. Dotyczy to rolników i domowników spełniających warunki z art. 7 ust. 1.\n",
      "3. Wyjątek określony w art. 7 ust. 4 ustawy o ubezpieczeniu społecznym rolników.\n",
      "4. Nie podlegają ubezpieczeniu, jeśli nie mają innego ubezpieczenia społecznego ani ustalonego prawa do świadczeń.\n",
      "5. Ubezpieczenie nie obejmuje wypadkowego, chorobowego i macierzyńskiego.\n",
      "6. Orzeczenie o niezdolności do samodzielnej egzystencji jest kluczowe dla wyłączenia z ubezpieczenia.\n",
      "\n",
      "(Jeśli chcesz dopytać, kliknij „Chciałbym dopytać”, a dodam kolejny ustęp do kontekstu.)\n",
      "\n",
      "\n",
      "\n",
      "[ROUTER] raw='jakie są warunki ubezpieczenia rolniczego?' | norm='jakie sa warunki ubezpieczenia rolniczego?' | ref=None\n",
      "\n",
      "Cytowane ustępy:\n",
      "- [ch2-art21-ust5] Rozdz.2 Art.21 Ust.5\n",
      "\n",
      "Odpowiedź:\n",
      "Warunki ubezpieczenia rolniczego obejmują uznanie ubezpieczonego za całkowicie niezdolnego do pracy w gospodarstwie rolnym z powodu naruszenia sprawności organizmu, co skutkuje utratą zdolności do osobistego wykonywania pracy w gospodarstwie. Kluczowe jest stwierdzenie takiej niezdolności przez odpowiednie organy. \n",
      "\n",
      "1. Utrata zdolności do osobistego wykonywania pracy w gospodarstwie rolnym.\n",
      "2. Naruszenie sprawności organizmu jako przyczyna niezdolności.\n",
      "3. Uznanie za całkowicie niezdolnego przez właściwe organy.\n",
      "4. Konieczność stwierdzenia niezdolności w celu uzyskania świadczeń z ubezpieczenia rolniczego.\n",
      "5. Brak możliwości wykonywania pracy w gospodarstwie rolnym osobiście.\n",
      "6. Zależność od orzeczeń dotyczących stanu zdrowia i zdolności do pracy.\n",
      "7. Warunki te są podstawą do przyznania świadczeń z tytułu ubezpieczenia rolniczego, np. renty rolniczej.\n",
      "8. Niezdolność musi być całkowita, a nie częściowa, aby kwalifikować się do pełnych świadczeń.\n",
      "\n",
      "(Jeśli chcesz dopytać, kliknij „Chciałbym dopytać”, a dodam kolejny ustęp do kontekstu.)\n",
      "\n",
      "\n",
      "\n",
      "[ROUTER] raw='jakie warunki trzeba spełnić, aby otrzymać ubezpieczenie rolnicze?' | norm='jakie warunki trzeba spełnic, aby otrzymac ubezpieczenie rolnicze?' | ref=None\n",
      "\n",
      "Cytowane ustępy:\n",
      "- [ch2-art19-ust2a] Rozdz.2 Art.19 Ust.2a\n",
      "\n",
      "Odpowiedź:\n",
      "Aby otrzymać ubezpieczenie rolnicze, należy spełnić warunki określone w art. 19 ust. 2 ustawy o ubezpieczeniu społecznym rolników. Kluczowe wymogi to:\n",
      "\n",
      "1. Prowadzenie gospodarstwa rolnego – posiadanie lub użytkowanie gruntów rolnych o powierzchni przekraczającej 1 ha przeliczeniowy.\n",
      "2. Zamieszkanie na terenie gospodarstwa – stałe zamieszkiwanie w gospodarstwie lub w jego bliskim sąsiedztwie.\n",
      "3. Wiek – ukończenie 18 lat i nieprzekroczenie wieku emerytalnego.\n",
      "4. Brak prawa do emerytury lub renty – nieposiadanie prawa do świadczeń z innych systemów ubezpieczeniowych.\n",
      "5. Spełnienie warunków do 31 grudnia 2017 r. – wymóg ten dotyczy osób, które złożyły wniosek przed tą datą.\n",
      "\n",
      "Warto zaznaczyć, że szczegółowe kryteria mogą się różnić w zależności od przepisów obowiązujących w danym okresie. W przypadku wątpliwości, należy skonsultować się z KRUS lub sprawdzić aktualne regulacje prawne.\n",
      "\n",
      "(Jeśli chcesz dopytać, kliknij „Chciałbym dopytać”, a dodam kolejny ustęp do kontekstu.)\n",
      "\n",
      "\n",
      "\n",
      "[ROUTER] raw='a jakie są warunki na emeryturę rolniczą?' | norm='a jakie sa warunki na emeryture rolnicza?' | ref=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cytowane ustępy:\n",
      "- [ch2-art17-ust4] Rozdz.2 Art.17 Ust.4\n",
      "\n",
      "Odpowiedź:\n",
      "Warunki uzyskania emerytury rolniczej obejmują:\n",
      "1. Wiek emerytalny – ukończenie 60 lat (kobiety) lub 65 lat (mężczyźni).\n",
      "2. Okres ubezpieczenia – co najmniej 25 lat opłacania składek na ubezpieczenie emerytalno-rentowe.\n",
      "3. Zaprzestanie prowadzenia działalności rolniczej – złożenie wniosku o emeryturę i zaprzestanie aktywnej działalności rolniczej.\n",
      "4. Wielkość gospodarstwa – nie ma bezpośredniego wpływu na prawo do emerytury, ale wpływa na wysokość składek.\n",
      "\n",
      "Dodatkowe informacje:\n",
      "- Wysokość składki zależy od wielkości gospodarstwa (patrz art. 17 ust. 4).\n",
      "- Emerytura rolnicza może być przyznana wcześniej w określonych przypadkach (np. niezdolność do pracy).\n",
      "\n",
      "(Jeśli chcesz dopytać, kliknij „Chciałbym dopytać”, a dodam kolejny ustęp do kontekstu.)\n",
      "\n",
      "\n",
      "\n",
      "[ROUTER] raw='co można ubezpieczyć w krus?' | norm='co mozna ubezpieczyc w krus?' | ref=None\n",
      "\n",
      "Cytowane ustępy:\n",
      "- [ch1-art5c-ust1] Rozdz.1 Art.5c Ust.1\n",
      "\n",
      "Odpowiedź:\n",
      "W KRUS można ubezpieczyć rolnika lub domownika, który podlega ubezpieczeniu w pełnym zakresie z mocy ustawy, ale został objęty innym ubezpieczeniem społecznym z tytułu:\n",
      "1. Pobierania świadczenia integracyjnego lub stypendium w okresie odbywania szkolenia, stażu lub przygotowania zawodowego dorosłych, na które został skierowany przez powiatowy urząd pracy.\n",
      "2. Pobierania stypendium w okresie odbywania szkolenia, stażu lub przygotowania zawodowego dorosłych, na które został skierowany przez inne niż powiatowy urząd pracy podmioty.\n",
      "3. Pobierania stypendium na podstawie przepisów o promocji zatrudnienia i instytucjach rynku pracy w okresie odbywania studiów podyplomowych.\n",
      "4. Pełnienia czynnej służby wojskowej jako żołnierz niezawodowy.\n",
      "5. Odbywania służby zastępczej.\n",
      "\n",
      "W tych przypadkach rolnik lub domownik nadal podlega ubezpieczeniu w KRUS, mimo że jest objęty innym ubezpieczeniem społecznym.\n",
      "\n",
      "Podsumowanie:\n",
      "KRUS obejmuje ubezpieczeniem rolników i domowników w określonych sytuacjach, nawet jeśli podlegają oni innemu ubezpieczeniu społecznemu. Dotyczy to przypadków związanych z pobieraniem świadczeń, stypendiów lub odbywaniem służby wojskowej/zastępczej.\n",
      "\n",
      "(Jeśli chcesz dopytać, kliknij „Chciałbym dopytać”, a dodam kolejny ustęp do kontekstu.)\n",
      "\n",
      "(\n",
      "\n",
      "[ROUTER] raw='a budynki można ubezpieczać?' | norm='a budynki mozna ubezpieczac?' | ref=None\n",
      "\n",
      "Cytowane ustępy:\n",
      "- [ch1-art5c-ust1] Rozdz.1 Art.5c Ust.1\n",
      "\n",
      "Odpowiedź:\n",
      "Tak, budynki można ubezpieczać. Ubezpieczenie budynków obejmuje zazwyczaj ochronę przed zdarzeniami losowymi, takimi jak pożar, powódź, gradobicie czy huragany. Polisy mogą również uwzględniać kradzieże lub uszkodzenia spowodowane przez wandalizm. W Polsce popularne są ubezpieczenia majątkowe oferowane przez towarzystwa ubezpieczeniowe, które chronią zarówno domy mieszkalne, jak i budynki gospodarcze (np. stodoły, garaże). Warto sprawdzić zakres ochrony i sumę ubezpieczenia, aby dostosować polisę do indywidualnych potrzeb.\n",
      "\n",
      "(Jeśli chcesz dopytać, kliknij „Chciałbym dopytać”, a dodam kolejny ustęp do kontekstu.)\n",
      "\n",
      "\n",
      "\n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# PROSTE CLI DO DOPYTAŃ\n",
    "# =======================\n",
    "\n",
    "def run_cli():\n",
    "    reset_context() \n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            q = input(\"> \").strip()\n",
    "            if not q:\n",
    "                continue\n",
    "            if q.lower() in (\"q\", \"quit\", \"exit\"):\n",
    "                print(\"Bye!\")\n",
    "                break\n",
    "            result = ask(q)\n",
    "            text = result[\"answer\"] if isinstance(result, dict) and \"answer\" in result else str(result)\n",
    "            print(\"\\n\" + text + \"\\n\")\n",
    "\n",
    "            while True:\n",
    "                dec = input(\"Czy chcesz dopytać? [t/n/q]: \").strip().lower()\n",
    "                if dec in (\"t\", \"tak\", \"y\", \"yes\"):\n",
    "                    want_follow_up()\n",
    "                    print(\"(\\n\")\n",
    "                    break  \n",
    "                elif dec in (\"n\", \"nie\", \"no\"):\n",
    "                    reset_context()\n",
    "                    print(\"\\n\")\n",
    "                    break\n",
    "                elif dec in (\"q\", \"quit\", \"exit\"):\n",
    "                    print(\"\\n\")\n",
    "                    return\n",
    "                else:\n",
    "                    print(\"\\n\")\n",
    "        except (KeyboardInterrupt, EOFError):\n",
    "            print(\"\\nBye!\")\n",
    "            break\n",
    "if __name__ == \"__main__\":\n",
    "    run_cli()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cab8c6e",
   "metadata": {},
   "source": [
    "# GRADIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f570491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_13776\\3517798818.py:249: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_13776\\3517798818.py:249: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚖️ CHATBOT PRAWNICZY KRUS - FAST MODE\n",
      "==================================================\n",
      "\n",
      "\n",
      "📊 Czas = ask() + ~1-2s (zamiast +70s)\n",
      "🚀 Uruchamiam na porcie 7882...\n",
      "* Running on local URL:  http://0.0.0.0:7890\n",
      "* Running on public URL: https://2610e3311768f1f754.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2610e3311768f1f754.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Zapisano pytanie: 'co to znaczy niezdolność do samodzielnej egzystencji?'\n",
      "DEBUG: Przetwarzam pytanie: 'co to znaczy niezdolność do samodzielnej egzystencji?'\n",
      "DEBUG: Start czasu: 1757415838.4218018\n",
      "[ROUTER] raw='co to znaczy niezdolność do samodzielnej egzystencji?' | norm='co to znaczy niezdolnosc do samodzielnej egzystencji?' | ref=None\n",
      "[CTX-JUDGE] dist={'NIE': 0.5041980147361755, 'TAK': 0.4958019554615021} margin=0.01\n",
      "[INTENT] judge_verdict=None conf=0.50\n",
      "DEBUG rerank scores\n",
      "(brak dokumentów)\n",
      "\n",
      "ODPOWIEDŹ\n",
      " Cytowane ustępy:\n",
      "(brak)\n",
      "\n",
      "Odpowiedź:\n",
      "Niezdolność do samodzielnej egzystencji to stan, w którym osoba wymaga stałej lub długotrwałej opieki i pomocy innych w zaspokajaniu podstawowych potrzeb życiowych. Obejmuje to m.in. zdolność do samodzielnego poruszania się, przygotowywania posiłków, dbania o higienę osobistą oraz załatwiania spraw urzędowych. W kontekście ubezpieczeń społecznych rolników, orzeczenie o niezdolności do samodzielnej egzystencji może wpływać na wysokość świadczeń lub dostęp do dodatkowych form pomocy. \n",
      "\n",
      "1. Definicja: Niezdolność do samodzielnej egzystencji oznacza brak możliwości samodzielnego funkcjonowania w codziennym życiu.\n",
      "2. Zakres potrzeb: Obejmuje podstawowe czynności życiowe, takie jak poruszanie się, higiena, przygotowywanie posiłków i załatwianie spraw urzędowych.\n",
      "3. Wpływ na świadczenia: Może wpływać na wysokość świadczeń z ubezpieczenia społecznego lub dostęp do dodatkowych form wsparcia.\n",
      "4. Opieka i pomoc: Wymaga stałej lub długotrwałej opieki i pomocy ze strony innych osób.\n",
      "5. Orzeczenie: Orzeczenie o niezdolności do samodzielnej egzystencji jest wydawane przez odpowiednie organy i ma znaczenie w kontekście ubezpieczeń społecznych.\n",
      "6. Konsekwencje: Osoby z takim orzeczeniem mogą mieć prawo do specjalnych świadczeń lub usług wsparcia.\n",
      "7. \n",
      "\n",
      " *Mogę popełniać błędy, skonsultuj się z placówką KRUS w celu potwierdzenia informacji.*\n",
      "\n",
      "DEBUG: ask() zakończone po 63.92s\n",
      "DEBUG: Długość odpowiedzi: 1287 znaków\n",
      "DEBUG: Formatowanie zakończone po 0.00s\n",
      "DEBUG: Całkowity czas: 63.92s\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Globalna zmienna do przechowania pytania\n",
    "current_question = \"\"\n",
    "\n",
    "def fast_answer(reset_memory, history):\n",
    "    global current_question\n",
    "    \n",
    "    print(f\"DEBUG: Przetwarzam pytanie: '{current_question}'\")\n",
    "    \n",
    "    if not current_question or not current_question.strip():\n",
    "        return history, \"💭 Oczekuję na pytanie prawnicze...\", '<div class=\"time-display\">-</div>'\n",
    "\n",
    "    # Pokaż loading state\n",
    "    loading_history = history + [(current_question, \"🔍 Analizuję przepisy prawne...\")]\n",
    "    \n",
    "    start = time.time()\n",
    "    print(f\"DEBUG: Start czasu: {start}\")\n",
    "\n",
    "    try:\n",
    "        # Wywołaj funkcję ask - tu jest właściwy czas przetwarzania\n",
    "        result = ask(current_question, reset_memory=reset_memory)\n",
    "        ask_time = time.time()\n",
    "        print(f\"DEBUG: ask() zakończone po {ask_time - start:.2f}s\")\n",
    "        \n",
    "        if isinstance(result, dict):\n",
    "            answer = result.get(\"answer\", str(result))\n",
    "        else:\n",
    "            answer = str(result)\n",
    "        \n",
    "        print(f\"DEBUG: Długość odpowiedzi: {len(answer)} znaków\")\n",
    "        \n",
    "        # Formatowanie odpowiedzi\n",
    "        formatted_answer = answer\n",
    "        if 'Cytowane ustępy:' in formatted_answer:\n",
    "            formatted_answer = formatted_answer.replace('Cytowane ustępy:', '📖 **Podstawa prawna:**')\n",
    "            formatted_answer = formatted_answer.replace('- [', '  📋 **[')\n",
    "            formatted_answer = formatted_answer.replace('] Rozdz.', ']** Rozdział ')\n",
    "            formatted_answer = formatted_answer.replace(' Art.', ' • Artykuł ')\n",
    "            formatted_answer = formatted_answer.replace(' Ust.', ' • Ustęp ')\n",
    "        \n",
    "        if 'Odpowiedź:' in formatted_answer:\n",
    "            formatted_answer = formatted_answer.replace('Odpowiedź:', '\\n💬 **Interpretacja prawna:**')\n",
    "        \n",
    "        # Wyróżnij kluczowe elementy prawne\n",
    "        formatted_answer = re.sub(r'\\bart\\.\\s*(\\d+)', r'**art. \\1**', formatted_answer, flags=re.IGNORECASE)\n",
    "        formatted_answer = re.sub(r'\\bust\\.\\s*(\\d+)', r'**ust. \\1**', formatted_answer, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Sprawdź czy końcówka nie została ucięta\n",
    "        if \"*Mogę popełniać błędy\" not in formatted_answer and \"skonsultuj się z\" not in formatted_answer:\n",
    "            formatted_answer += \"\\n\\n*Mogę popełniać błędy, skonsultuj się z placówką KRUS w celu potwierdzenia informacji.*\"\n",
    "        \n",
    "        format_time = time.time()\n",
    "        print(f\"DEBUG: Formatowanie zakończone po {format_time - ask_time:.2f}s\")\n",
    "\n",
    "        # Oblicz całkowity czas\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start\n",
    "        print(f\"DEBUG: Całkowity czas: {total_time:.2f}s\")\n",
    "        \n",
    "        # Zwróć finalną odpowiedź\n",
    "        final_history = history + [(current_question, formatted_answer)]\n",
    "        final_status = \"✅ Analiza prawna zakończona!\"\n",
    "        time_display = f'<div class=\"time-display success\">{total_time:.2f}s</div>'\n",
    "        \n",
    "        return final_history, final_status, time_display\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        \n",
    "        error_msg = f\"❌ **Błąd analizy prawnej:** {str(e)}\\n\\nSpróbuj przeformułować pytanie.\"\n",
    "        error_history = history + [(current_question, error_msg)]\n",
    "        error_status = \"❌ Wystąpił błąd podczas analizy\"\n",
    "        error_time = '<div class=\"time-display error\">❌</div>'\n",
    "        \n",
    "        return error_history, error_status, error_time\n",
    "\n",
    "# Minimalistyczny jasny theme - identyczny jak poprzednio\n",
    "with gr.Blocks(\n",
    "    theme=gr.themes.Soft(\n",
    "        primary_hue=gr.themes.colors.blue,\n",
    "        secondary_hue=gr.themes.colors.gray,\n",
    "        neutral_hue=gr.themes.colors.gray,\n",
    "        font=gr.themes.GoogleFont(\"Inter\")\n",
    "    ).set(\n",
    "        background_fill_primary=\"#ffffff\",\n",
    "        background_fill_secondary=\"#f8fafc\",\n",
    "        border_color_primary=\"#e2e8f0\",\n",
    "        button_primary_background_fill=\"linear-gradient(90deg, #3b82f6, #2563eb)\",\n",
    "        button_primary_background_fill_hover=\"linear-gradient(90deg, #2563eb, #1d4ed8)\",\n",
    "        button_secondary_background_fill=\"#f1f5f9\",\n",
    "        button_secondary_background_fill_hover=\"#e2e8f0\",\n",
    "        button_secondary_text_color=\"#475569\",\n",
    "        input_background_fill=\"#ffffff\",\n",
    "        input_border_color=\"#cbd5e1\"\n",
    "    ),\n",
    "    css=\"\"\"\n",
    "    .gradio-container { \n",
    "        max-width: 1400px !important; \n",
    "        font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;\n",
    "    }\n",
    "    \n",
    "    /* Minimalistyczny header */\n",
    "    .header {\n",
    "        background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%);\n",
    "        border-radius: 12px;\n",
    "        padding: 32px;\n",
    "        margin-bottom: 24px;\n",
    "        border: 1px solid #cbd5e1;\n",
    "        text-align: center;\n",
    "    }\n",
    "    \n",
    "    /* Status panel */\n",
    "    .status-panel { \n",
    "        background: #f1f5f9;\n",
    "        color: #475569;\n",
    "        padding: 12px 16px;\n",
    "        border-radius: 8px;\n",
    "        text-align: center;\n",
    "        font-weight: 500;\n",
    "        margin: 8px 0;\n",
    "        border: 1px solid #e2e8f0;\n",
    "        font-size: 14px;\n",
    "    }\n",
    "    \n",
    "    /* Time display - wyróżniony */\n",
    "    .time-display {\n",
    "        background: #ffffff;\n",
    "        border: 2px solid #e2e8f0;\n",
    "        border-radius: 12px;\n",
    "        padding: 20px;\n",
    "        text-align: center;\n",
    "        margin: 12px 0;\n",
    "        font-size: 32px;\n",
    "        font-weight: 700;\n",
    "        color: #64748b;\n",
    "        min-height: 85px;\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "        justify-content: center;\n",
    "        transition: all 0.3s ease;\n",
    "        font-family: 'Inter', monospace;\n",
    "    }\n",
    "    \n",
    "    .time-display.processing {\n",
    "        border-color: #f59e0b;\n",
    "        color: #d97706;\n",
    "        background: #fffbeb;\n",
    "        animation: pulse 2s infinite;\n",
    "    }\n",
    "    \n",
    "    .time-display.success {\n",
    "        border-color: #10b981;\n",
    "        color: #059669;\n",
    "        background: #f0fdf4;\n",
    "    }\n",
    "    \n",
    "    .time-display.error {\n",
    "        border-color: #ef4444;\n",
    "        color: #dc2626;\n",
    "        background: #fef2f2;\n",
    "    }\n",
    "    \n",
    "    @keyframes pulse {\n",
    "        0%, 100% { opacity: 1; }\n",
    "        50% { opacity: 0.7; }\n",
    "    }\n",
    "    \n",
    "    /* Example buttons */\n",
    "    .example-btn { \n",
    "        margin: 3px;\n",
    "        border-radius: 8px;\n",
    "        transition: all 0.2s ease;\n",
    "        background: #ffffff;\n",
    "        border: 1px solid #e2e8f0;\n",
    "        color: #475569;\n",
    "        font-size: 13px;\n",
    "        padding: 8px 12px;\n",
    "    }\n",
    "    \n",
    "    .example-btn:hover { \n",
    "        background: #f8fafc;\n",
    "        border-color: #3b82f6;\n",
    "        color: #1e40af;\n",
    "        transform: translateY(-1px);\n",
    "        box-shadow: 0 4px 12px rgba(59, 130, 246, 0.15);\n",
    "    }\n",
    "    \n",
    "    /* Tech panel */\n",
    "    .tech-panel {\n",
    "        background: #f8fafc;\n",
    "        border: 1px solid #e2e8f0;\n",
    "        border-radius: 12px;\n",
    "        padding: 20px;\n",
    "        margin: 20px 0;\n",
    "    }\n",
    "    \n",
    "    /* Performance panel */\n",
    "    .perf-panel {\n",
    "        background: #f0fdf4;\n",
    "        border: 1px solid #bbf7d0;\n",
    "        border-radius: 12px;\n",
    "        padding: 16px;\n",
    "        margin: 16px 0;\n",
    "    }\n",
    "    \n",
    "    /* Category headers */\n",
    "    .category-header {\n",
    "        color: #475569;\n",
    "        font-weight: 600;\n",
    "        margin: 20px 0 12px 0;\n",
    "        font-size: 15px;\n",
    "        padding-bottom: 6px;\n",
    "        border-bottom: 2px solid #e2e8f0;\n",
    "    }\n",
    "    \n",
    "    /* Footer */\n",
    "    .footer {\n",
    "        background: #f8fafc;\n",
    "        border-radius: 12px;\n",
    "        border: 1px solid #e2e8f0;\n",
    "        padding: 20px;\n",
    "        margin-top: 32px;\n",
    "        text-align: center;\n",
    "    }\n",
    "    \"\"\",\n",
    "    title=\"Chatbot Prawniczy KRUS\"\n",
    ") as demo:\n",
    "    \n",
    "    # Header\n",
    "    gr.HTML(\"\"\"\n",
    "    <div class=\"header\">\n",
    "        <h1 style=\"margin: 0; font-size: 2.4em; font-weight: 600; color: #1e40af;\">\n",
    "            ⚖️ Chatbot Prawniczy KRUS\n",
    "        </h1>\n",
    "        <p style=\"margin: 12px 0 0 0; font-size: 1.1em; color: #64748b; font-weight: 400;\">\n",
    "            System Analizy Prawnej\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        # Kolumna główna - Chat\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"💬 Konsultacja prawnicza\",\n",
    "                height=520,\n",
    "                bubble_full_width=False,\n",
    "                show_copy_button=True\n",
    "            )\n",
    "            \n",
    "            status_display = gr.HTML(\n",
    "                value='<div class=\"status-panel\">💭 System gotowy do analizy prawnej</div>'\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                user_input = gr.Textbox(\n",
    "                    label=\"Pytanie prawnicze\",\n",
    "                    placeholder=\"Np: Jak oblicza się wysokość emerytury rolniczej?\",\n",
    "                    lines=2,\n",
    "                    scale=4\n",
    "                )\n",
    "                send_btn = gr.Button(\"🚀 Analizuj\", variant=\"primary\", size=\"lg\", scale=1)\n",
    "            \n",
    "            with gr.Row():\n",
    "                reset_memory = gr.Checkbox(\n",
    "                    label=\"🔄 Reset kontekstu\", \n",
    "                    value=False,\n",
    "                    info=\"Wyczyść pamięć poprzednich pytań\"\n",
    "                )\n",
    "                clear_btn = gr.Button(\"🗑️ Nowa sesja\", variant=\"secondary\")\n",
    "        \n",
    "        # Kolumna boczna\n",
    "        with gr.Column(scale=2):\n",
    "            gr.HTML('<h3 style=\"margin: 20px 0 15px 0; color: #475569; font-weight: 600;\">⚡ Czas odpowiedzi</h3>')\n",
    "            \n",
    "            response_time_display = gr.HTML('<div class=\"time-display\">Oczekiwanie...</div>')\n",
    "            \n",
    "            # Przykładowe pytania\n",
    "            gr.HTML('<h3 style=\"margin: 32px 0 20px 0; color: #475569; font-weight: 600;\">💡 Przykłady</h3>')\n",
    "            \n",
    "            gr.HTML('<h4 class=\"category-header\">🏛️ Emerytury</h4>')\n",
    "            emerytura_examples = [\n",
    "                \"Warunki nabycia prawa do emerytury rolniczej\",\n",
    "                \"Wymagania wiekowe dla emerytury\", \n",
    "                \"Obliczanie wysokości emerytury\"\n",
    "            ]\n",
    "            \n",
    "            for example in emerytura_examples:\n",
    "                btn = gr.Button(f\"📋 {example}\", variant=\"secondary\", size=\"sm\", elem_classes=[\"example-btn\"])\n",
    "                btn.click(fn=lambda q=example: q, outputs=user_input)\n",
    "            \n",
    "            gr.HTML('<h4 class=\"category-header\">🛡️ Ubezpieczenia</h4>')\n",
    "            ubezpieczenia_examples = [\n",
    "                \"Kto podlega ubezpieczeniu w KRUS?\",\n",
    "                \"Składki na ubezpieczenie rolników\",\n",
    "                \"Definicja gospodarstwa rolnego\"\n",
    "            ]\n",
    "            \n",
    "            for example in ubezpieczenia_examples:\n",
    "                btn = gr.Button(f\"📋 {example}\", variant=\"secondary\", size=\"sm\", elem_classes=[\"example-btn\"])\n",
    "                btn.click(fn=lambda q=example: q, outputs=user_input)\n",
    "            \n",
    "            gr.HTML('<h4 class=\"category-header\">💰 Świadczenia</h4>')\n",
    "            swiadczenia_examples = [\n",
    "                \"Prawo do renty rodzinnej\",\n",
    "                \"Dokumenty przy wniosku o świadczenia\", \n",
    "                \"Okres wypłaty zasiłku macierzyńskiego\"\n",
    "            ]\n",
    "            \n",
    "            for example in swiadczenia_examples:\n",
    "                btn = gr.Button(f\"📋 {example}\", variant=\"secondary\", size=\"sm\", elem_classes=[\"example-btn\"])\n",
    "                btn.click(fn=lambda q=example: q, outputs=user_input)\n",
    "            \n",
    "            # Panel techniczny\n",
    "            gr.HTML(\"\"\"\n",
    "            <div class=\"tech-panel\">\n",
    "                <h3 style=\"margin: 0 0 12px 0; color: #374151; font-weight: 600; font-size: 16px;\">🔧 Optymalizacja</h3>\n",
    "                <div style=\"font-size: 12px; line-height: 1.6; color: #64748b;\">\n",
    "                    <strong>Tryb:</strong><br>\n",
    "                    <strong>Overhead:</strong> Zminimalizowany<br>\n",
    "                    <strong>Czas:</strong> ask() + ~1-2s Gradio<br>\n",
    "                    <strong>Queue:</strong> Wyłączona<br>\n",
    "                    <strong>Network:</strong> 1 request zamiast 100+\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\")\n",
    "            \n",
    "            # Wskaźniki\n",
    "            gr.HTML(\"\"\"\n",
    "            <div class=\"perf-panel\">\n",
    "                <h4 style=\"color: #065f46; margin: 0 0 8px 0; font-weight: 600; font-size: 14px;\">📈 Wydajność</h4>\n",
    "                <div style=\"font-size: 11px; color: #059669; line-height: 1.5;\">\n",
    "                    <strong>Czas:</strong> ~20-25s (vs 90s ze streamowaniem)<br>\n",
    "                    <strong>Requests:</strong> 1 (vs 100+)<br>\n",
    "                    <strong>CPU:</strong> Minimalne obciążenie\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\")\n",
    "    \n",
    "    # Funkcje obsługi - BEZ KOLEJKI I GENERATORÓW\n",
    "    def save_question_and_clear(msg, history):\n",
    "        global current_question\n",
    "        current_question = msg.strip()\n",
    "        print(f\"DEBUG: Zapisano pytanie: '{current_question}'\")\n",
    "        return history, \"\"\n",
    "\n",
    "    def clear_chat():\n",
    "        global current_question\n",
    "        current_question = \"\"\n",
    "        return [], \"💭 System zresetowany\", '<div class=\"time-display\">-</div>'\n",
    "    \n",
    "    # Event binding - WSZYSTKO BEZ QUEUE\n",
    "    send_btn.click(\n",
    "        fn=save_question_and_clear,\n",
    "        inputs=[user_input, chatbot],\n",
    "        outputs=[chatbot, user_input],\n",
    "        queue=False\n",
    "    ).then(\n",
    "        fn=fast_answer,  # Funkcja bez generatora!\n",
    "        inputs=[reset_memory, chatbot],\n",
    "        outputs=[chatbot, status_display, response_time_display],\n",
    "        queue=False  # Kluczowe - bez kolejki\n",
    "    )\n",
    "    \n",
    "    user_input.submit(\n",
    "        fn=save_question_and_clear,\n",
    "        inputs=[user_input, chatbot],\n",
    "        outputs=[chatbot, user_input],\n",
    "        queue=False\n",
    "    ).then(\n",
    "        fn=fast_answer,\n",
    "        inputs=[reset_memory, chatbot],\n",
    "        outputs=[chatbot, status_display, response_time_display],\n",
    "        queue=False\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(\n",
    "        fn=clear_chat,\n",
    "        outputs=[chatbot, status_display, response_time_display],\n",
    "        queue=False\n",
    "    )\n",
    "    \n",
    "    # Footer\n",
    "    gr.HTML(\"\"\"\n",
    "    <div class=\"footer\">\n",
    "        <p style=\"margin: 0 0 8px 0; font-weight: 600; color: #374151;\">\n",
    "            ⚖️ Chatbot Prawniczy KRUS v2.0 - Fast Mode\n",
    "        </p>\n",
    "        <p style=\"font-size: 11px; color: #64748b; margin: 0; line-height: 1.4;\">\n",
    "            Bez streamowania = prawdziwy czas odpowiedzi. System służy celom informacyjnym.\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n",
    "# Uruchomienie BEZ KOLEJKI\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"⚖️ CHATBOT PRAWNICZY KRUS - FAST MODE\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"📊 Czas = ask() + ~1-2s (zamiast +70s)\")\n",
    "    print(\"🚀 Uruchamiam na porcie 7882...\")\n",
    "    \n",
    "    # BEZ demo.queue() - to było główne źródło opóźnień\n",
    "    demo.launch(\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7890,\n",
    "        share=True,\n",
    "        show_error=True,\n",
    "        inbrowser=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
