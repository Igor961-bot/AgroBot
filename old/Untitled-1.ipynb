{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9b0e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()   # zwalnia pamięć zaalokowaną w cache\n",
    "torch.cuda.ipc_collect()   # dodatkowe czyszczenie pamięci współdzielonej\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"KRUS-debug\"\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"\")\n",
    "\n",
    "\n",
    "sorDEBUG = True\n",
    "DEBUG = sorDEBUG\n",
    "RETURN_STRING_WHEN_DEBUG_FALSE = True\n",
    "\n",
    "\n",
    "DATA_PATH = \"C:/Users/admin/Desktop/krus-chatbot/AgroBot/data/ustawa_with_paragraph_headers.md\"\n",
    "PERSIST_PATH = \"chroma_ustawa\"\n",
    "\n",
    "EMBEDDER_MODEL   = \"intfloat/multilingual-e5-small\"\n",
    "RERANKER_MODEL   = \"radlab/polish-cross-encoder\"\n",
    "CLASSIFIER_MODEL = \"klasyfikator\" \n",
    "BASE_MODEL_ID    = \"speakleash/Bielik-11B-v2.6-Instruct\"\n",
    "#\"CYFRAGOVPL/PLLuM-12B-chat\"\n",
    "\n",
    "RERANK_THRESHOLD = 0.30\n",
    "K_SIM   = 10\n",
    "K_FINAL = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fd8dd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0.dev20250903+cu128 | CUDA available: True | GPUs: 2\n",
      "Platform: Windows-11-10.0.26100-SP0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f525d60732414e1484194bba08b15512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oba modele załadowane bez kwantyzacji\n"
     ]
    }
   ],
   "source": [
    "import os, torch, platform\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"   \n",
    "torch.backends.cuda.matmul.allow_tf32 = True    \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available(), \"| GPUs:\", torch.cuda.device_count())\n",
    "print(\"Platform:\", platform.platform()) \n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=True, trust_remote_code=False)\n",
    "except Exception as e:\n",
    "    print(\"Fast tokenizer fail → slow:\", e)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=False)\n",
    "\n",
    "try:\n",
    "    classifier_tok = AutoTokenizer.from_pretrained(CLASSIFIER_MODEL, use_fast=True, trust_remote_code=False)\n",
    "except Exception as e:\n",
    "    print(\"Fast classifier tokenizer fail → slow:\", e)\n",
    "    classifier_tok = AutoTokenizer.from_pretrained(CLASSIFIER_MODEL, use_fast=False)\n",
    "\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,      \n",
    "    device_map=\"cuda:0\",               \n",
    "    low_cpu_mem_usage=False,\n",
    "    attn_implementation=\"sdpa\",      \n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "\n",
    "clf_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CLASSIFIER_MODEL,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "\n",
    "print(\"oba modele załadowane bez kwantyzacji\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a4ec07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INDEX] Liczba dokumentów (ustępów): 444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_13776\\2375576860.py:49: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceBgeEmbeddings(\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_13776\\2375576860.py:62: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "import shutil, re\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "persist_path = PERSIST_PATH\n",
    "shutil.rmtree(persist_path, ignore_errors=True)\n",
    "os.makedirs(persist_path, exist_ok=True)\n",
    "\n",
    "docs_raw = TextLoader(DATA_PATH, encoding=\"utf-8\").load()\n",
    "header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"###\", \"ustep\")])\n",
    "chunks = header_splitter.split_text(docs_raw[0].page_content)\n",
    "\n",
    "# meta: <!-- chapter:1 article:16a paragraph:3 id:ch1-art16a-ust3 -->\n",
    "meta_re = re.compile(\n",
    "    r\"<!--\\s*chapter\\s*:\\s*(\\d+)\\s+article\\s*:\\s*([0-9a-z]+)\\s+paragraph\\s*:\\s*([0-9a-z]+)\\s+id\\s*:\\s*([^\\s>]+)\\s*-->\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "normed: List[Document] = []\n",
    "for d in chunks:\n",
    "    md = dict(d.metadata)\n",
    "    header_text = md.get(\"ustep\", \"\") or \"\"\n",
    "    source_for_meta = header_text + \"\\n\" + d.page_content\n",
    "    m = meta_re.search(source_for_meta)\n",
    "    if m:\n",
    "        md[\"chapter\"]   = int(m.group(1))\n",
    "        md[\"article\"]   = m.group(2).lower()\n",
    "        md[\"paragraph\"] = m.group(3).lower()\n",
    "        md[\"id\"]        = m.group(4)\n",
    "        md[\"rozdzial\"]  = md[\"chapter\"]\n",
    "        md[\"artykul\"]   = md[\"article\"]\n",
    "        md[\"ust\"]       = md[\"paragraph\"]\n",
    "\n",
    "    clean_header = meta_re.sub(\"\", header_text).strip()\n",
    "    if clean_header:\n",
    "        md[\"ustep\"] = clean_header\n",
    "\n",
    "    content = meta_re.sub(\"\", d.page_content).strip()\n",
    "    normed.append(Document(page_content=content, metadata=md))\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"[INDEX] Liczba dokumentów (ustępów): {len(normed)}\")\n",
    "\n",
    "embedder = HuggingFaceBgeEmbeddings(\n",
    "    model_name=EMBEDDER_MODEL,\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "shutil.rmtree(persist_path, ignore_errors=True)\n",
    "db = Chroma.from_documents(\n",
    "    documents=normed,\n",
    "    embedding=embedder,\n",
    "    persist_directory=persist_path,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "db.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b3291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:196: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     97\u001b[39m                 all_scores.extend(scores.tolist())\n\u001b[32m     99\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.array(all_scores)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m cross_encoder = \u001b[43mOptimizedONNXCrossEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRERANKER_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m REF_RE_EXT = re.compile(\n\u001b[32m    105\u001b[39m     \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(?:art\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.?\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*(?P<art>[0-9]+[a-z]?))\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m     \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(?:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*(?:ust(?:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.|ęp)?|ustep)\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*(?P<ust>[0-9]+[a-z]?))?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m     re.IGNORECASE\n\u001b[32m    110\u001b[39m )\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_ref_ext\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) -> Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mOptimizedONNXCrossEncoder.__init__\u001b[39m\u001b[34m(self, model_name, device)\u001b[39m\n\u001b[32m     57\u001b[39m providers = [\u001b[33m\"\u001b[39m\u001b[33mCPUExecutionProvider\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mORTModelForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mONNX Cross Encoder załadowany na CPU!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optimum\\onnxruntime\\modeling_ort.py:552\u001b[39m, in \u001b[36mORTModel.from_pretrained\u001b[39m\u001b[34m(cls, model_id, config, export, subfolder, revision, force_download, local_files_only, trust_remote_code, cache_dir, token, provider, providers, provider_options, session_options, use_io_binding, **kwargs)\u001b[39m\n\u001b[32m    547\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    548\u001b[39m         logger.warning(\n\u001b[32m    549\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`file_name` was set to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` but will be ignored as the model will be converted to ONNX\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    550\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_io_binding\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_io_binding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optimum\\modeling_base.py:419\u001b[39m, in \u001b[36mOptimizedModel.from_pretrained\u001b[39m\u001b[34m(cls, model_id, config, export, subfolder, revision, force_download, local_files_only, trust_remote_code, cache_dir, token, **kwargs)\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    417\u001b[39m     from_pretrained_method = \u001b[38;5;28mcls\u001b[39m._from_pretrained\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_pretrained_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hub options\u001b[39;49;00m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optimum\\onnxruntime\\modeling_ort.py:422\u001b[39m, in \u001b[36mORTModel._export\u001b[39m\u001b[34m(cls, model_id, config, subfolder, revision, force_download, local_files_only, trust_remote_code, cache_dir, token, **kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m model_save_dir = TemporaryDirectory()\n\u001b[32m    420\u001b[39m model_save_path = Path(model_save_dir.name)\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m \u001b[43mmain_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_validation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mno_post_process\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_library_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m maybe_save_preprocessors(model_id, model_save_path, src_subfolder=subfolder)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._from_pretrained(model_save_path, config, model_save_dir=model_save_dir, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optimum\\exporters\\onnx\\__main__.py:418\u001b[39m, in \u001b[36mmain_export\u001b[39m\u001b[34m(model_name_or_path, output, task, opset, device, dtype, fp16, optimize, monolith, no_post_process, framework, atol, pad_token_id, subfolder, revision, force_download, local_files_only, trust_remote_code, cache_dir, token, for_ort, do_validation, model_kwargs, custom_onnx_configs, fn_get_submodels, use_subprocess, _variant, library_name, legacy, no_dynamic_axes, do_constant_folding, slim, **kwargs_shapes)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;66;03m# The preprocessors are loaded as they may be useful to export the model. Notably, some of the static input shapes may be stored in the\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[38;5;66;03m# preprocessors config.\u001b[39;00m\n\u001b[32m    414\u001b[39m preprocessors = maybe_load_preprocessors(\n\u001b[32m    415\u001b[39m     model_name_or_path, subfolder=subfolder, trust_remote_code=trust_remote_code\n\u001b[32m    416\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m \u001b[43monnx_export_from_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonolith\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmonolith\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mno_post_process\u001b[49m\u001b[43m=\u001b[49m\u001b[43mno_post_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_validation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_onnx_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_onnx_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn_get_submodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfn_get_submodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_variant\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_variant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlegacy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlegacy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocessors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocessors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mno_dynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mno_dynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_subprocess\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_subprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mslim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mslim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optimum\\exporters\\onnx\\convert.py:1186\u001b[39m, in \u001b[36monnx_export_from_model\u001b[39m\u001b[34m(model, output, opset, optimize, monolith, no_post_process, atol, do_validation, model_kwargs, custom_onnx_configs, fn_get_submodels, _variant, legacy, preprocessors, device, no_dynamic_axes, task, use_subprocess, do_constant_folding, slim, **kwargs_shapes)\u001b[39m\n\u001b[32m   1181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m float_dtype == \u001b[33m\"\u001b[39m\u001b[33mbf16\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1182\u001b[39m     logger.warning(\n\u001b[32m   1183\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExporting the model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in bfloat16 float dtype. After the export, ONNX Runtime InferenceSession with CPU/CUDA execution provider likely does not implement all operators for the bfloat16 data type, and the loading is likely to fail.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1184\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1186\u001b[39m _, onnx_outputs = \u001b[43mexport_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodels_and_onnx_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels_and_onnx_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43monnx_files_subpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfloat_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mno_dynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mno_dynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optimize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1200\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01monnxruntime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoOptimizationConfig, ORTOptimizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optimum\\exporters\\onnx\\convert.py:770\u001b[39m, in \u001b[36mexport_models\u001b[39m\u001b[34m(models_and_onnx_configs, output_dir, opset, output_names, device, input_shapes, disable_dynamic_axes_fix, dtype, no_dynamic_axes, do_constant_folding, model_kwargs)\u001b[39m\n\u001b[32m    764\u001b[39m     output_path.parent.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    766\u001b[39m     logger.info(\n\u001b[32m    767\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m***** Exporting submodel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(models_and_onnx_configs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubmodel.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m *****\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    768\u001b[39m     )\n\u001b[32m    769\u001b[39m     outputs.append(\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m         \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43msub_onnx_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdisable_dynamic_axes_fix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_dynamic_axes_fix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m            \u001b[49m\u001b[43mno_dynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mno_dynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    783\u001b[39m     )\n\u001b[32m    785\u001b[39m outputs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mzip\u001b[39m(*outputs)))\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optimum\\exporters\\onnx\\convert.py:874\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(model, config, output, opset, device, input_shapes, disable_dynamic_axes_fix, dtype, no_dynamic_axes, do_constant_folding, model_kwargs)\u001b[39m\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.is_torch_support_available:\n\u001b[32m    870\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m MinimumVersionError(\n\u001b[32m    871\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported PyTorch version for this model. Minimum required is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.MIN_TORCH_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_torch_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    872\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m874\u001b[39m     export_output = \u001b[43mexport_pytorch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mno_dynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mno_dynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(model), TFPreTrainedModel):\n\u001b[32m    887\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optimum\\exporters\\onnx\\convert.py:579\u001b[39m, in \u001b[36mexport_pytorch\u001b[39m\u001b[34m(model, config, opset, output, device, input_shapes, no_dynamic_axes, do_constant_folding, model_kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     onnx_export(\n\u001b[32m    568\u001b[39m         model,\n\u001b[32m    569\u001b[39m         (dummy_inputs,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    575\u001b[39m         opset_version=opset,\n\u001b[32m    576\u001b[39m     )\n\u001b[32m    578\u001b[39m \u001b[38;5;66;03m# check if external data was exported\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m onnx_model = \u001b[43monnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_external_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m model_uses_external_data = check_model_uses_external_data(onnx_model)\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_uses_external_data \u001b[38;5;129;01mor\u001b[39;00m FORCE_ONNX_EXTERNAL_DATA:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\onnx\\__init__.py:226\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(f, format, load_external_data)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model\u001b[39m(\n\u001b[32m    206\u001b[39m     f: IO[\u001b[38;5;28mbytes\u001b[39m] | \u001b[38;5;28mstr\u001b[39m | os.PathLike,\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mformat\u001b[39m: _SupportedFormat | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# noqa: A002\u001b[39;00m\n\u001b[32m    208\u001b[39m     load_external_data: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    209\u001b[39m ) -> ModelProto:\n\u001b[32m    210\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Loads a serialized ModelProto into memory.\u001b[39;00m\n\u001b[32m    211\u001b[39m \n\u001b[32m    212\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    224\u001b[39m \u001b[33;03m        Loaded in-memory ModelProto.\u001b[39;00m\n\u001b[32m    225\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     model = _get_serializer(\u001b[38;5;28mformat\u001b[39m, f).deserialize_proto(\u001b[43m_load_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m, ModelProto())\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m load_external_data:\n\u001b[32m    229\u001b[39m         model_filepath = _get_file_path(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\onnx\\__init__.py:164\u001b[39m, in \u001b[36m_load_bytes\u001b[39m\u001b[34m(f)\u001b[39m\n\u001b[32m    162\u001b[39m     f = typing.cast(\u001b[33m\"\u001b[39m\u001b[33mUnion[str, os.PathLike]\u001b[39m\u001b[33m\"\u001b[39m, f)\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m readable:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m         content = \u001b[43mreadable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "K_SIM = globals().get(\"K_SIM\", 12)\n",
    "K_FINAL = globals().get(\"K_FINAL\", 6)\n",
    "RERANK_THRESHOLD = globals().get(\"RERANK_THRESHOLD\", None) \n",
    "RERANKER_MODEL = globals().get(\"RERANKER_MODEL\", \"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "import os, re, shutil, unicodedata, asyncio\n",
    "from typing import List, Optional, Dict, Any, Tuple, Callable\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.callbacks import CallbackManager\n",
    "from langchain.callbacks.tracers.langchain import LangChainTracer\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.documents import Document  \n",
    "\n",
    "# Reranker + LLM\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import pipeline as hf_pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import onnxruntime as ort\n",
    "\n",
    "def strip_accents_lower(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    return s.lower().strip()\n",
    "\n",
    "\n",
    "if \"db\" not in globals():\n",
    "    raise RuntimeError(\"Brak globalnej bazy `db` (Chroma). Zainicjalizuj ją przed załadowaniem skryptu.\")\n",
    "\n",
    "model = gen_model\n",
    "\n",
    "ROLE_CUT_RE = re.compile(\n",
    "    r\"(?i)\"                              \n",
    "    r\"(###\\s*(?:user|asystent|dokumenty|system)?\\s*:?\" \n",
    "    r\"|(?:^|\\s)(?:user|asystent|dokumenty|system)\\s*:\" \n",
    "    r\"|<\\s*(?:user|assistant|docs?|system)\\s*>)\"      \n",
    ")\n",
    "\n",
    "def cut_after_role_markers(s: str) -> str:\n",
    "    if not s:\n",
    "        return s\n",
    "    m = ROLE_CUT_RE.search(s)\n",
    "    return s[:m.start()].rstrip() if m else s\n",
    "\n",
    "\n",
    "if globals().get(\"model\", None) is None or globals().get(\"tokenizer\", None) is None:\n",
    "    raise RuntimeError(\"Załaduj wcześniej LLM do zmiennych `model` i `tokenizer`.\")\n",
    "\n",
    "# reranker z ONNX na CPU\n",
    "device = \"cpu\"\n",
    "\n",
    "class OptimizedONNXCrossEncoder:\n",
    "    def __init__(self, model_name, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        providers = [\"CPUExecutionProvider\"]\n",
    "        \n",
    "        try:\n",
    "            self.model = ORTModelForSequenceClassification.from_pretrained(\n",
    "                model_name,\n",
    "                provider=providers[0],\n",
    "                export=True\n",
    "            )\n",
    "            print(\"ONNX Cross Encoder załadowany na CPU!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fallback do zwykłego CrossEncoder na CPU: {e}\")\n",
    "            from sentence_transformers import CrossEncoder\n",
    "            self.fallback_model = CrossEncoder(model_name, device=\"cpu\")\n",
    "            self.model = None\n",
    "    \n",
    "    def predict(self, pairs, batch_size=32, **kwargs):\n",
    "        if self.model is None:\n",
    "            return self.fallback_model.predict(pairs, batch_size=batch_size)\n",
    "        \n",
    "        all_scores = []\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = pairs[i:i+batch_size]\n",
    "            texts_1 = [p[0] for p in batch]\n",
    "            texts_2 = [p[1] for p in batch]\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                texts_1, texts_2,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                if outputs.logits.shape[-1] == 2:\n",
    "                    scores = torch.softmax(outputs.logits, dim=-1)[:, 1]\n",
    "                else:\n",
    "                    scores = outputs.logits[:, 0]\n",
    "                scores = scores.cpu().numpy()\n",
    "                all_scores.extend(scores.tolist())\n",
    "        \n",
    "        return np.array(all_scores)\n",
    "\n",
    "cross_encoder = OptimizedONNXCrossEncoder(RERANKER_MODEL, device=device)\n",
    "\n",
    "\n",
    "REF_RE_EXT = re.compile(\n",
    "    r\"(?:art\\.?\\s*(?P<art>[0-9]+[a-z]?))\"\n",
    "    r\"(?:\\s*(?:ust(?:\\.|ęp)?|ustep)\\s*(?P<ust>[0-9]+[a-z]?))?\"\n",
    "    r\"(?:\\s*(?:pkt\\.?)\\s*(?P<pkt>[0-9]+[a-z]?))?\"\n",
    "    r\"(?:\\s*(?:lit\\.?)\\s*(?P<lit>[a-z]))?\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def parse_ref_ext(query: str) -> Optional[Dict[str, str]]:\n",
    "    m = REF_RE_EXT.search(query or \"\")\n",
    "    if not m:\n",
    "        return None\n",
    "    ref = {}\n",
    "    if m.group(\"art\"): ref[\"article\"]   = m.group(\"art\").lower()\n",
    "    if m.group(\"ust\"): ref[\"paragraph\"] = m.group(\"ust\").lower()\n",
    "    if m.group(\"pkt\"): ref[\"punkt\"]     = m.group(\"pkt\").lower()\n",
    "    if m.group(\"lit\"): ref[\"litera\"]    = m.group(\"lit\").lower()\n",
    "    return ref if ref else None\n",
    "\n",
    "#retriever + reranker\n",
    "def retrieve_basic(query: str, k_sim: int = K_SIM, k_final: int = K_FINAL, rerank_threshold: float | None = RERANK_THRESHOLD) -> List[Document]:\n",
    "    ref = parse_ref_ext(query)\n",
    "    filter_dict = {}\n",
    "    if ref:\n",
    "        if \"article\" in ref:   filter_dict[\"article\"]   = ref[\"article\"]\n",
    "        if \"paragraph\" in ref: filter_dict[\"paragraph\"] = ref[\"paragraph\"]\n",
    "    if not filter_dict:\n",
    "        filter_dict = None\n",
    "\n",
    "    docs = db.similarity_search(query, k=k_sim, filter=filter_dict)\n",
    "    if not docs:\n",
    "        return []\n",
    "\n",
    "    pairs = [(query, d.page_content) for d in docs]\n",
    "    scores = cross_encoder.predict(pairs, batch_size=32) \n",
    "    scores = np.asarray(scores, dtype=float)\n",
    "\n",
    "    scored_docs = [(d, s) for d, s in zip(docs, scores)]\n",
    "    if rerank_threshold is not None:\n",
    "        scored_docs = [(d, s) for d, s in scored_docs if s >= rerank_threshold]\n",
    "        if not scored_docs:\n",
    "            return []\n",
    "\n",
    "    scored_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)[:k_final]\n",
    "    out: List[Document] = []\n",
    "    for d, s in scored_docs:\n",
    "        md = dict(d.metadata or {})\n",
    "        md[\"rerank_score\"] = float(s)\n",
    "        d.metadata = md\n",
    "        out.append(d)\n",
    "    return out\n",
    "\n",
    "#memory + LLM\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=3, memory_key=\"chat_history\", return_messages=True, output_key=\"answer\"\n",
    ")\n",
    "\n",
    "hf_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=globals().get(\"model\"),\n",
    "    tokenizer=globals().get(\"tokenizer\"),\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.35,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.0,\n",
    "    pad_token_id=(getattr(globals().get(\"tokenizer\"), \"eos_token_id\", None)),\n",
    "    eos_token_id=(getattr(globals().get(\"tokenizer\"), \"eos_token_id\", None)),\n",
    "    return_full_text=False,\n",
    "    use_cache=True,\n",
    "    batch_size=1,  \n",
    "    num_beams=1,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipe)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"### System:\\n\"\n",
    "        \"Jesteś ekspertem prawa ubezpieczeń społecznych rolników. \"\n",
    "        \"Odpowiadasz WYŁĄCZNIE na podstawie Dokumentów poniżej. \"\n",
    "        \"Twoim zadaniem jest odpowiedzenie na pytanie najlepiej jak możesz wyłącznie na podstawie Dokumentów. \"\n",
    "        \"Nie zmyślaj informacji i jeśli w dokumentach brak odpowiedzi, poinformuj o tym.\\n\"\n",
    "        \"Formatowanie: nie używaj formatowania Markdown (**…**) ani __…__.\\n\"\n",
    "        \"Napisz maksymalnie 6-8 zdań lub 8 punktów.\"\n",
    "        \"### User:\\n{question}\\n\\n\"\n",
    "        \"### Dokumenty:\\n{context}\\n\"\n",
    "        \"### Asystent:\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "tracer = LangChainTracer()\n",
    "callback_manager = CallbackManager([tracer])\n",
    "\n",
    "class FunctionRetriever(BaseRetriever):\n",
    "    k_sim: int\n",
    "    k_final: int\n",
    "    rerank_threshold: Optional[float] = None\n",
    "    _fn: Callable[..., List[Document]] = PrivateAttr()\n",
    "\n",
    "    def __init__(self, fn: Callable[..., List[Document]], **data):\n",
    "        super().__init__(**data)\n",
    "        self._fn = fn\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self._fn(query, k_sim=self.k_sim, k_final=self.k_final, rerank_threshold=self.rerank_threshold)\n",
    "\n",
    "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(None, lambda: self._get_relevant_documents(query))\n",
    "\n",
    "init_retriever = FunctionRetriever(fn=retrieve_basic, k_sim=K_SIM, k_final=K_FINAL, rerank_threshold=RERANK_THRESHOLD)\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=init_retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,\n",
    "    output_key=\"answer\",\n",
    "    callback_manager=callback_manager\n",
    ")\n",
    "\n",
    "#formatowanie odpowiedzi\n",
    "def _build_citations_block(docs: List[Document]) -> str:\n",
    "    if not docs:\n",
    "        return \"Cytowane ustępy:\\n(brak)\\n\"\n",
    "    lines = [\"Cytowane ustępy:\"]\n",
    "    for d in docs:\n",
    "        md = d.metadata or {}\n",
    "        rozdz = md.get(\"rozdzial\", md.get(\"chapter\"))\n",
    "        art   = md.get(\"artykul\",  md.get(\"article\"))\n",
    "        ust   = md.get(\"ust\",      md.get(\"paragraph\"))\n",
    "        pid   = md.get(\"id\", f\"ch{rozdz}-art{art}-ust{ust}\")\n",
    "        lines.append(f\"- [{pid}] Rozdz.{rozdz} Art.{art} Ust.{ust}\")\n",
    "    return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "def format_docs_for_prompt(docs: List[Document]) -> str:\n",
    "    blocks = []\n",
    "    for d in docs:\n",
    "        md = d.metadata or {}\n",
    "        rozdz = md.get(\"rozdzial\", md.get(\"chapter\"))\n",
    "        art   = md.get(\"artykul\",  md.get(\"article\"))\n",
    "        ust   = md.get(\"ust\",      md.get(\"paragraph\"))\n",
    "        pid   = md.get(\"id\", f\"ch{rozdz}-art{art}-ust{ust}\")\n",
    "        blocks.append(f\"[{pid}]\\n{d.page_content}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "def log_rerank_scores(docs: List[Document], header: str = \"DEBUG rerank scores\") -> None:\n",
    "    if not DEBUG: return\n",
    "    print(header)\n",
    "    if not docs:\n",
    "        print(\"(brak dokumentów)\"); return\n",
    "    for d in docs:\n",
    "        md = d.metadata or {}\n",
    "        pid = md.get(\"id\") or f\"ch{md.get('chapter')}-art{md.get('article')}-ust{md.get('paragraph')}\"\n",
    "        sc  = md.get(\"rerank_score\")\n",
    "        print(f\"- {pid}: score={sc:.6f}\" if isinstance(sc, (int, float)) else f\"- {pid}: score=(brak)\")\n",
    "\n",
    "\n",
    "def strip_markdown_bold(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    text = re.sub(r\"\\*\\*(.*?)\\*\\*\", r\"\\1\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"__(.*?)__\", r\"\\1\", text, flags=re.DOTALL)\n",
    "    text = text.replace(\"**\", \"\").replace(\"__\", \"\")\n",
    "    return text\n",
    "\n",
    "_SENT_ENDERS = \".?!…\"\n",
    "_CLOSERS = \"”»\\\")]’\"\n",
    "\n",
    "def _rstrip_u(s: str) -> str:\n",
    "    return s.rstrip(\" \\t\\r\\n\\u00A0\")\n",
    "\n",
    "_LIST_MARKER_ONLY_RE = re.compile(\n",
    "    r\"\"\"^[\\s\\u00A0]*(\n",
    "            [-*+•·—–]\n",
    "          | (?:\\(?\\d+[a-z]?\\)|\\d+[a-z]?[.)])\n",
    "          | (?:\\(?[ivxlcdm]+\\)|[ivxlcdm]+[.)])\n",
    "          | (?:[a-z][.)])\n",
    "        )[\\s\\u00A0]*$\"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE\n",
    ")\n",
    "\n",
    "def _strip_trailing_empty_list_item(s: str) -> tuple[str, bool]:\n",
    "    if not s:\n",
    "        return s, False\n",
    "    s2 = _rstrip_u(s)\n",
    "    if not s2:\n",
    "        return s2, (s2 != s)\n",
    "    lines = s2.splitlines()\n",
    "    last = _rstrip_u(lines[-1])\n",
    "    if _LIST_MARKER_ONLY_RE.match(last or \"\"):\n",
    "        return _rstrip_u(\"\\n\".join(lines[:-1])), True\n",
    "    return s2, False\n",
    "\n",
    "def _ends_with_full_stop(s: str) -> bool:\n",
    "    return re.search(rf\"[{re.escape(_SENT_ENDERS)}][{re.escape(_CLOSERS)}]*[\\s\\u00A0]*$\", s) is not None\n",
    "\n",
    "_ABBR_SET = {\"art\",\"ust\",\"pkt\",\"lit\",\"tj\",\"tzw\",\"np\",\"itd\",\"itp\",\"m.in\",\"prof\",\"dr\",\"nr\",\"poz\",\"cd\",\"al\",\"ul\",\"pl\",\"św\",\"sw\"}\n",
    "\n",
    "def _last_token_before_dot_for_trim(buf: str) -> str:\n",
    "    m = re.search(r\"([A-Za-zÀ-ÖØ-öø-ÿŁŚŻŹĆŃÓÄÖÜĄĘłśżźćńóäöüąę]+)\\.\\s*$\", buf)\n",
    "    return (m.group(1).lower() if m else \"\")\n",
    "\n",
    "def _is_abbreviation_dot(text: str, dot_pos: int) -> bool:\n",
    "    prev = text[:dot_pos+1]\n",
    "    tok = _last_token_before_dot_for_trim(prev)\n",
    "    return tok in _ABBR_SET\n",
    "\n",
    "def _find_last_safe_boundary(s: str) -> int | None:\n",
    "    i = len(s) - 1\n",
    "    while i >= 0 and (s[i].isspace() or s[i] in _CLOSERS or s[i] == \"\\u00A0\"):\n",
    "        i -= 1\n",
    "    while i >= 0:\n",
    "        ch = s[i]\n",
    "        if ch in _SENT_ENDERS:\n",
    "            if ch == \".\" and _is_abbreviation_dot(s[:i+1], i):\n",
    "                i -= 1\n",
    "                continue\n",
    "            return i + 1\n",
    "        i -= 1\n",
    "    return None\n",
    "\n",
    "def trim_incomplete_sentences(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    text = cut_after_role_markers(text)\n",
    "    s = _rstrip_u(text)\n",
    "    changed = True\n",
    "    while changed:\n",
    "        s, changed = _strip_trailing_empty_list_item(s)\n",
    "    if s.endswith(\":\"):\n",
    "        last_nl = s.rfind(\"\\n\")\n",
    "        s = _rstrip_u(s[:last_nl]) if last_nl != -1 else \"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    if _ends_with_full_stop(s):\n",
    "        return s\n",
    "    cut = _find_last_safe_boundary(s)\n",
    "    if cut is None:\n",
    "        return s\n",
    "    return _rstrip_u(s[:cut])\n",
    "\n",
    "def _finalize_return(text: str, docs: List[Document], mode: str):\n",
    "    debug = [{\"id\": (d.metadata or {}).get(\"id\"),\n",
    "              \"score\": (d.metadata or {}).get(\"rerank_score\")} for d in (docs or [])]\n",
    "    payload = {\"answer\": text, \"source_documents\": docs, \"debug\": {\"mode\": mode, \"rerank\": debug}}\n",
    "\n",
    "    if DEBUG:\n",
    "        print(\"\\nODPOWIEDŹ\\n\", text,\n",
    "              \"\\n\\n *Mogę popełniać błędy, skonsultuj się z placówką KRUS w celu potwierdzenia informacji.*\\n\")\n",
    "        return payload\n",
    "\n",
    "    if RETURN_STRING_WHEN_DEBUG_FALSE:\n",
    "        return text\n",
    "    return payload\n",
    "\n",
    "_SMALLTALK_RULES = [\n",
    "    (r\"^(czesc|cze|hej|heja|hejka|witam|siema|elo|halo|dzien dobry|dobry wieczor)\\b\",\n",
    "     \"Cześć! W czym mogę pomóc w sprawie KRUS/ustawy?\"),\n",
    "    (r\"\\b(dzieki|dziekuje|dzieki wielkie|dziekuje bardzo|thx|thanks)\\b\",\n",
    "     \"Nie ma sprawy! Jeśli chcesz, podaj kolejne pytanie.\"),\n",
    "]\n",
    "def smalltalk_reply(user_q: str) -> Optional[str]:\n",
    "    qn = strip_accents_lower(user_q)\n",
    "    for pat, resp in _SMALLTALK_RULES:\n",
    "        if re.search(pat, qn, flags=re.IGNORECASE):\n",
    "            return resp\n",
    "    return None\n",
    "\n",
    "#context judge\n",
    "CTX_JUDGE_MODEL = \"joeddav/xlm-roberta-large-xnli\"\n",
    "ctx_judge = hf_pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=CTX_JUDGE_MODEL,\n",
    "    device=1 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "CTX_YES_MIN = 0.60\n",
    "CTX_NO_MIN  = 0.60\n",
    "CTX_MARGIN  = 0.12\n",
    "\n",
    "def judge_contextual(prev_q: str, curr_q: str) -> Tuple[Optional[bool], float, Dict[str, float]]:\n",
    "    if not prev_q or not curr_q:\n",
    "        return None, 0.0, {}\n",
    "    seq = (\n",
    "        f\"Pytanie A: {prev_q}\\n\"\n",
    "        f\"Pytanie B: {curr_q}\\n\"\n",
    "        f\"Czy Pytanie B jest dopytaniem/uzupełnieniem/jest powiązane do Pytania A?\"\n",
    "    )\n",
    "    out = ctx_judge(\n",
    "        sequences=seq,\n",
    "        candidate_labels=[\"TAK\", \"NIE\"],\n",
    "        hypothesis_template=\"To jest {}.\"\n",
    "    )\n",
    "    dist = {lbl.upper(): float(score) for lbl, score in zip(out[\"labels\"], out[\"scores\"])}\n",
    "    yes, no = dist.get(\"TAK\", 0.0), dist.get(\"NIE\", 0.0)\n",
    "    margin = abs(yes - no)\n",
    "    if DEBUG:\n",
    "        print(f\"[CTX-JUDGE] dist={dist} margin={margin:.2f}\")\n",
    "    if yes >= CTX_YES_MIN and margin >= CTX_MARGIN:\n",
    "        return True, yes, dist\n",
    "    if no >= CTX_NO_MIN and margin >= CTX_MARGIN:\n",
    "        return False, no, dist\n",
    "    return None, max(yes, no), dist\n",
    "\n",
    "#conversation state\n",
    "class ConversationState:\n",
    "    def __init__(self):\n",
    "        self.last_article_num: Optional[str] = None\n",
    "        self.last_paragraph_num: Optional[str] = None\n",
    "        self.last_docs: List[Document] = []\n",
    "        self.last_query: Optional[str] = None\n",
    "\n",
    "STATE = ConversationState()\n",
    "\n",
    "def _short_doc_label(d: Document) -> str:\n",
    "    md = d.metadata or {}\n",
    "    rozdz = md.get(\"rozdzial\", md.get(\"chapter\"))\n",
    "    art   = md.get(\"artykul\",  md.get(\"article\"))\n",
    "    ust   = md.get(\"ust\",      md.get(\"paragraph\"))\n",
    "    pid   = md.get(\"id\", f\"ch{rozdz}-art{art}-ust{ust}\")\n",
    "    return f\"{pid}\"\n",
    "\n",
    "def _update_state_from_docs(docs: List[Document], user_q: str):\n",
    "    if not docs: return\n",
    "    STATE.last_docs = docs[:]\n",
    "    STATE.last_query = user_q\n",
    "    md0 = docs[0].metadata or {}\n",
    "    STATE.last_article_num   = (md0.get(\"artykul\")  or md0.get(\"article\"))\n",
    "    STATE.last_paragraph_num = (md0.get(\"ust\")      or md0.get(\"paragraph\"))\n",
    "    if DEBUG:\n",
    "        print(f\"[STATE] last_article={STATE.last_article_num} last_paragraph={STATE.last_paragraph_num}\")\n",
    "\n",
    "def rewrite_query(user_q: str) -> str:\n",
    "    base = (STATE.last_query or \"\").strip()\n",
    "    if not base: return user_q\n",
    "    doc_labels = [_short_doc_label(d) for d in (STATE.last_docs or [])][:6]\n",
    "    doc_part = f\" (odnieś do: {', '.join(doc_labels)})\" if doc_labels else \"\"\n",
    "    return f\"{user_q} — w kontekście poprzedniego pytania: '{base}'{doc_part}\"\n",
    "\n",
    "#router\n",
    "def route_query(query: str):\n",
    "    qn = strip_accents_lower(query)\n",
    "    ref = parse_ref_ext(query)\n",
    "    if DEBUG:\n",
    "        print(f\"[ROUTER] raw='{query}' | norm='{qn}' | ref={ref}\")\n",
    "    if ref:\n",
    "        return \"EXPLICIT_REF\", {\"ref\": ref}\n",
    "    return \"GENERAL\", {\"query\": query}\n",
    "\n",
    "#restricted answer from docs\n",
    "def _llm_generate(prompt_tmpl: PromptTemplate, **kwargs) -> str:\n",
    "    text = prompt_tmpl.format(**kwargs)\n",
    "    out = llm.invoke(text)\n",
    "    return (out or \"\").strip()\n",
    "\n",
    "def answer_from_docs(question: str, docs: List[Document]):\n",
    "    ctx = format_docs_for_prompt(docs) if docs else \"(brak dokumentów)\"\n",
    "    answer = _llm_generate(prompt, question=question, context=ctx)\n",
    "    answer = trim_incomplete_sentences(strip_markdown_bold(answer)) or answer\n",
    "    final_text = f\"{_build_citations_block(docs)}\\nOdpowiedź:\\n{answer}\"\n",
    "    return _finalize_return(final_text, docs, mode=\"restricted\")\n",
    "\n",
    "#main function\n",
    "def ask(q: str, reset_memory: bool=False):\n",
    "    if reset_memory:\n",
    "        qa_chain.memory.clear()\n",
    "\n",
    "    st = smalltalk_reply(q)\n",
    "    if st is not None:\n",
    "        return _finalize_return(st, [], mode=\"smalltalk\")\n",
    "\n",
    "    action, payload = route_query(q)\n",
    "    if action == \"EXPLICIT_REF\":\n",
    "        ref = payload[\"ref\"]\n",
    "        flt = {}\n",
    "        if \"article\" in ref:   flt[\"article\"]   = ref[\"article\"]\n",
    "        if \"paragraph\" in ref: flt[\"paragraph\"] = ref[\"paragraph\"]\n",
    "        docs = db.similarity_search(\"treść przepisu\", k=5, filter=flt)\n",
    "        if DEBUG:\n",
    "            print(\"[ROUTER] EXPLICIT_REF → filter\", flt, \"→ docs:\", [ (d.metadata or {}).get(\"id\") for d in docs ])\n",
    "        if docs:\n",
    "            _update_state_from_docs(docs, q)\n",
    "            content = strip_markdown_bold(docs[0].page_content or \"\")\n",
    "            content = trim_incomplete_sentences(content) or content\n",
    "            final_text = f\"{_build_citations_block(docs)}\\nOdpowiedź (pełny przepis):\\n{content}\"\n",
    "            return _finalize_return(final_text, docs, mode=\"explicit\")\n",
    "        else:\n",
    "            return _finalize_return(\"Nie znalazłem takiego artykułu/ustępu.\", [], mode=\"explicit\")\n",
    "\n",
    "    verdict, conf, dist = judge_contextual(STATE.last_query or \"\", q)\n",
    "    if DEBUG:\n",
    "        print(f\"[INTENT] judge_verdict={verdict} conf={conf:.2f}\")\n",
    "\n",
    "    mode = \"follow_up\" if verdict is True else \"new_query\"  \n",
    "\n",
    "    if mode == \"follow_up\":\n",
    "        rewritten = rewrite_query(q)\n",
    "        if DEBUG: print(f\"[REWRITE:follow_up] {rewritten}\")\n",
    "\n",
    "        docs_narrow: List[Document] = []\n",
    "        if STATE.last_article_num:\n",
    "            flt = {\"article\": STATE.last_article_num}\n",
    "            docs_tmp = db.similarity_search(rewritten, k=K_SIM, filter=flt)\n",
    "            if docs_tmp:\n",
    "                pairs = [(rewritten, d.page_content) for d in docs_tmp]\n",
    "                scores = cross_encoder.predict(pairs, batch_size=32)\n",
    "                scored = sorted([(d, float(s)) for d, s in zip(docs_tmp, np.asarray(scores))],\n",
    "                                key=lambda x: x[1], reverse=True)[:K_FINAL]\n",
    "                for d, s in scored:\n",
    "                    md = dict(d.metadata or {}); md[\"rerank_score\"] = s; d.metadata = md\n",
    "                    docs_narrow.append(d)\n",
    "\n",
    "        if not docs_narrow and STATE.last_docs:\n",
    "            STATE.last_query = q\n",
    "            return answer_from_docs(rewritten, STATE.last_docs)\n",
    "\n",
    "        if not docs_narrow:\n",
    "            res = qa_chain.invoke({\"question\": rewritten})\n",
    "            out_docs = res.get(\"source_documents\", []) or []\n",
    "            _update_state_from_docs(out_docs, q)\n",
    "            log_rerank_scores(out_docs)\n",
    "            citations_block = _build_citations_block(out_docs)\n",
    "            raw_answer = (res.get(\"answer\") or \"\").strip()\n",
    "            raw_answer = trim_incomplete_sentences(strip_markdown_bold(raw_answer)) or raw_answer\n",
    "            final_text = f\"{citations_block}\\nOdpowiedź:\\n{raw_answer}\"\n",
    "            return _finalize_return(final_text, out_docs, mode=\"follow_up→global\")\n",
    "\n",
    "        _update_state_from_docs(docs_narrow, q)\n",
    "        return answer_from_docs(rewritten, docs_narrow)\n",
    "\n",
    "    res = qa_chain.invoke({\"question\": q})\n",
    "    docs = res.get(\"source_documents\", []) or []\n",
    "    _update_state_from_docs(docs, q)\n",
    "    log_rerank_scores(docs)\n",
    "\n",
    "    citations_block = _build_citations_block(docs)\n",
    "    raw_answer = (res.get(\"answer\") or \"\").strip()\n",
    "    raw_answer = trim_incomplete_sentences(strip_markdown_bold(raw_answer)) or raw_answer\n",
    "    final_text = f\"{citations_block}\\nOdpowiedź:\\n{raw_answer}\"\n",
    "    return _finalize_return(final_text, docs, mode=\"new_query\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "daf3476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ROUTER] raw='Co to oznacza niezdolność do samodzielnej egzystencji?' | norm='co to oznacza niezdolnosc do samodzielnej egzystencji?' | ref=None\n",
      "[CTX-JUDGE] dist={'NIE': 0.5373283624649048, 'TAK': 0.46267157793045044} margin=0.07\n",
      "[INTENT] judge_verdict=None conf=0.54\n",
      "[STATE] last_article=7 last_paragraph=4\n",
      "DEBUG rerank scores\n",
      "- ch2-art7-ust4: score=0.562168\n",
      "\n",
      "ODPOWIEDŹ\n",
      " Cytowane ustępy:\n",
      "- [ch2-art7-ust4] Rozdz.2 Art.7 Ust.4\n",
      "\n",
      "Odpowiedź:\n",
      "Niezdolność do samodzielnej egzystencji oznacza stan, w którym dana osoba wymaga stałej lub długotrwałej opieki i pomocy innych w zaspokajaniu podstawowych potrzeb życiowych. Obejmuje to m.in. zdolność do samodzielnego poruszania się, przygotowywania posiłków, dbania o higienę osobistą oraz załatwiania spraw urzędowych. Osoby z takim orzeczeniem często potrzebują wsparcia 24/7, co uniemożliwia im samodzielne funkcjonowanie w codziennym życiu. W kontekście ubezpieczeń społecznych, orzeczenie o niezdolności do samodzielnej egzystencji może wpływać na wysokość świadczeń lub dostęp do dodatkowych form pomocy. \n",
      "\n",
      " *Mogę popełniać błędy, skonsultuj się z placówką KRUS w celu potwierdzenia informacji.*\n",
      "\n",
      "{'answer': 'Cytowane ustępy:\\n- [ch2-art7-ust4] Rozdz.2 Art.7 Ust.4\\n\\nOdpowiedź:\\nNiezdolność do samodzielnej egzystencji oznacza stan, w którym dana osoba wymaga stałej lub długotrwałej opieki i pomocy innych w zaspokajaniu podstawowych potrzeb życiowych. Obejmuje to m.in. zdolność do samodzielnego poruszania się, przygotowywania posiłków, dbania o higienę osobistą oraz załatwiania spraw urzędowych. Osoby z takim orzeczeniem często potrzebują wsparcia 24/7, co uniemożliwia im samodzielne funkcjonowanie w codziennym życiu. W kontekście ubezpieczeń społecznych, orzeczenie o niezdolności do samodzielnej egzystencji może wpływać na wysokość świadczeń lub dostęp do dodatkowych form pomocy.', 'source_documents': [Document(metadata={'article': '7', 'paragraph': '4', 'rozdzial': 2, 'ustep': 'Ust. 4', 'artykul': '7', 'id': 'ch2-art7-ust4', 'ust': '4', 'chapter': 2, 'rerank_score': 0.5621684193611145}, page_content='Przepisów ust. 2 i 3 nie stosuje się do emerytów i rencistów, którzy mają orzeczoną niezdolność do samodzielnej egzystencji.')], 'debug': {'mode': 'new_query', 'rerank': [{'id': 'ch2-art7-ust4', 'score': 0.5621684193611145}]}}\n"
     ]
    }
   ],
   "source": [
    "print(ask(\"Co to oznacza niezdolność do samodzielnej egzystencji?\", reset_memory=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa040da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU count: 2\n",
      "0 NVIDIA GeForce RTX 5090\n",
      "1 NVIDIA GeForce RTX 5090\n",
      "GPU0: allocated=0.00 GB, reserved=0.00 GB\n",
      "GPU1: allocated=0.00 GB, reserved=0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))\n",
    "\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    alloc = torch.cuda.memory_allocated(i)/1e9\n",
    "    res   = torch.cuda.memory_reserved(i)/1e9\n",
    "    print(f\"GPU{i}: allocated={alloc:.2f} GB, reserved={res:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cab8c6e",
   "metadata": {},
   "source": [
    "# GRADIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f570491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_13776\\3517798818.py:249: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_13776\\3517798818.py:249: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚖️ CHATBOT PRAWNICZY KRUS - FAST MODE\n",
      "==================================================\n",
      "\n",
      "\n",
      "📊 Czas = ask() + ~1-2s (zamiast +70s)\n",
      "🚀 Uruchamiam na porcie 7882...\n",
      "* Running on local URL:  http://0.0.0.0:7890\n",
      "* Running on public URL: https://2610e3311768f1f754.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2610e3311768f1f754.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Zapisano pytanie: 'co to znaczy niezdolność do samodzielnej egzystencji?'\n",
      "DEBUG: Przetwarzam pytanie: 'co to znaczy niezdolność do samodzielnej egzystencji?'\n",
      "DEBUG: Start czasu: 1757415838.4218018\n",
      "[ROUTER] raw='co to znaczy niezdolność do samodzielnej egzystencji?' | norm='co to znaczy niezdolnosc do samodzielnej egzystencji?' | ref=None\n",
      "[CTX-JUDGE] dist={'NIE': 0.5041980147361755, 'TAK': 0.4958019554615021} margin=0.01\n",
      "[INTENT] judge_verdict=None conf=0.50\n",
      "DEBUG rerank scores\n",
      "(brak dokumentów)\n",
      "\n",
      "ODPOWIEDŹ\n",
      " Cytowane ustępy:\n",
      "(brak)\n",
      "\n",
      "Odpowiedź:\n",
      "Niezdolność do samodzielnej egzystencji to stan, w którym osoba wymaga stałej lub długotrwałej opieki i pomocy innych w zaspokajaniu podstawowych potrzeb życiowych. Obejmuje to m.in. zdolność do samodzielnego poruszania się, przygotowywania posiłków, dbania o higienę osobistą oraz załatwiania spraw urzędowych. W kontekście ubezpieczeń społecznych rolników, orzeczenie o niezdolności do samodzielnej egzystencji może wpływać na wysokość świadczeń lub dostęp do dodatkowych form pomocy. \n",
      "\n",
      "1. Definicja: Niezdolność do samodzielnej egzystencji oznacza brak możliwości samodzielnego funkcjonowania w codziennym życiu.\n",
      "2. Zakres potrzeb: Obejmuje podstawowe czynności życiowe, takie jak poruszanie się, higiena, przygotowywanie posiłków i załatwianie spraw urzędowych.\n",
      "3. Wpływ na świadczenia: Może wpływać na wysokość świadczeń z ubezpieczenia społecznego lub dostęp do dodatkowych form wsparcia.\n",
      "4. Opieka i pomoc: Wymaga stałej lub długotrwałej opieki i pomocy ze strony innych osób.\n",
      "5. Orzeczenie: Orzeczenie o niezdolności do samodzielnej egzystencji jest wydawane przez odpowiednie organy i ma znaczenie w kontekście ubezpieczeń społecznych.\n",
      "6. Konsekwencje: Osoby z takim orzeczeniem mogą mieć prawo do specjalnych świadczeń lub usług wsparcia.\n",
      "7. \n",
      "\n",
      " *Mogę popełniać błędy, skonsultuj się z placówką KRUS w celu potwierdzenia informacji.*\n",
      "\n",
      "DEBUG: ask() zakończone po 63.92s\n",
      "DEBUG: Długość odpowiedzi: 1287 znaków\n",
      "DEBUG: Formatowanie zakończone po 0.00s\n",
      "DEBUG: Całkowity czas: 63.92s\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Globalna zmienna do przechowania pytania\n",
    "current_question = \"\"\n",
    "\n",
    "def fast_answer(reset_memory, history):\n",
    "    global current_question\n",
    "    \n",
    "    print(f\"DEBUG: Przetwarzam pytanie: '{current_question}'\")\n",
    "    \n",
    "    if not current_question or not current_question.strip():\n",
    "        return history, \"💭 Oczekuję na pytanie prawnicze...\", '<div class=\"time-display\">-</div>'\n",
    "\n",
    "    # Pokaż loading state\n",
    "    loading_history = history + [(current_question, \"🔍 Analizuję przepisy prawne...\")]\n",
    "    \n",
    "    start = time.time()\n",
    "    print(f\"DEBUG: Start czasu: {start}\")\n",
    "\n",
    "    try:\n",
    "        # Wywołaj funkcję ask - tu jest właściwy czas przetwarzania\n",
    "        result = ask(current_question, reset_memory=reset_memory)\n",
    "        ask_time = time.time()\n",
    "        print(f\"DEBUG: ask() zakończone po {ask_time - start:.2f}s\")\n",
    "        \n",
    "        if isinstance(result, dict):\n",
    "            answer = result.get(\"answer\", str(result))\n",
    "        else:\n",
    "            answer = str(result)\n",
    "        \n",
    "        print(f\"DEBUG: Długość odpowiedzi: {len(answer)} znaków\")\n",
    "        \n",
    "        # Formatowanie odpowiedzi\n",
    "        formatted_answer = answer\n",
    "        if 'Cytowane ustępy:' in formatted_answer:\n",
    "            formatted_answer = formatted_answer.replace('Cytowane ustępy:', '📖 **Podstawa prawna:**')\n",
    "            formatted_answer = formatted_answer.replace('- [', '  📋 **[')\n",
    "            formatted_answer = formatted_answer.replace('] Rozdz.', ']** Rozdział ')\n",
    "            formatted_answer = formatted_answer.replace(' Art.', ' • Artykuł ')\n",
    "            formatted_answer = formatted_answer.replace(' Ust.', ' • Ustęp ')\n",
    "        \n",
    "        if 'Odpowiedź:' in formatted_answer:\n",
    "            formatted_answer = formatted_answer.replace('Odpowiedź:', '\\n💬 **Interpretacja prawna:**')\n",
    "        \n",
    "        # Wyróżnij kluczowe elementy prawne\n",
    "        formatted_answer = re.sub(r'\\bart\\.\\s*(\\d+)', r'**art. \\1**', formatted_answer, flags=re.IGNORECASE)\n",
    "        formatted_answer = re.sub(r'\\bust\\.\\s*(\\d+)', r'**ust. \\1**', formatted_answer, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Sprawdź czy końcówka nie została ucięta\n",
    "        if \"*Mogę popełniać błędy\" not in formatted_answer and \"skonsultuj się z\" not in formatted_answer:\n",
    "            formatted_answer += \"\\n\\n*Mogę popełniać błędy, skonsultuj się z placówką KRUS w celu potwierdzenia informacji.*\"\n",
    "        \n",
    "        format_time = time.time()\n",
    "        print(f\"DEBUG: Formatowanie zakończone po {format_time - ask_time:.2f}s\")\n",
    "\n",
    "        # Oblicz całkowity czas\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start\n",
    "        print(f\"DEBUG: Całkowity czas: {total_time:.2f}s\")\n",
    "        \n",
    "        # Zwróć finalną odpowiedź\n",
    "        final_history = history + [(current_question, formatted_answer)]\n",
    "        final_status = \"✅ Analiza prawna zakończona!\"\n",
    "        time_display = f'<div class=\"time-display success\">{total_time:.2f}s</div>'\n",
    "        \n",
    "        return final_history, final_status, time_display\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        \n",
    "        error_msg = f\"❌ **Błąd analizy prawnej:** {str(e)}\\n\\nSpróbuj przeformułować pytanie.\"\n",
    "        error_history = history + [(current_question, error_msg)]\n",
    "        error_status = \"❌ Wystąpił błąd podczas analizy\"\n",
    "        error_time = '<div class=\"time-display error\">❌</div>'\n",
    "        \n",
    "        return error_history, error_status, error_time\n",
    "\n",
    "# Minimalistyczny jasny theme - identyczny jak poprzednio\n",
    "with gr.Blocks(\n",
    "    theme=gr.themes.Soft(\n",
    "        primary_hue=gr.themes.colors.blue,\n",
    "        secondary_hue=gr.themes.colors.gray,\n",
    "        neutral_hue=gr.themes.colors.gray,\n",
    "        font=gr.themes.GoogleFont(\"Inter\")\n",
    "    ).set(\n",
    "        background_fill_primary=\"#ffffff\",\n",
    "        background_fill_secondary=\"#f8fafc\",\n",
    "        border_color_primary=\"#e2e8f0\",\n",
    "        button_primary_background_fill=\"linear-gradient(90deg, #3b82f6, #2563eb)\",\n",
    "        button_primary_background_fill_hover=\"linear-gradient(90deg, #2563eb, #1d4ed8)\",\n",
    "        button_secondary_background_fill=\"#f1f5f9\",\n",
    "        button_secondary_background_fill_hover=\"#e2e8f0\",\n",
    "        button_secondary_text_color=\"#475569\",\n",
    "        input_background_fill=\"#ffffff\",\n",
    "        input_border_color=\"#cbd5e1\"\n",
    "    ),\n",
    "    css=\"\"\"\n",
    "    .gradio-container { \n",
    "        max-width: 1400px !important; \n",
    "        font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;\n",
    "    }\n",
    "    \n",
    "    /* Minimalistyczny header */\n",
    "    .header {\n",
    "        background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%);\n",
    "        border-radius: 12px;\n",
    "        padding: 32px;\n",
    "        margin-bottom: 24px;\n",
    "        border: 1px solid #cbd5e1;\n",
    "        text-align: center;\n",
    "    }\n",
    "    \n",
    "    /* Status panel */\n",
    "    .status-panel { \n",
    "        background: #f1f5f9;\n",
    "        color: #475569;\n",
    "        padding: 12px 16px;\n",
    "        border-radius: 8px;\n",
    "        text-align: center;\n",
    "        font-weight: 500;\n",
    "        margin: 8px 0;\n",
    "        border: 1px solid #e2e8f0;\n",
    "        font-size: 14px;\n",
    "    }\n",
    "    \n",
    "    /* Time display - wyróżniony */\n",
    "    .time-display {\n",
    "        background: #ffffff;\n",
    "        border: 2px solid #e2e8f0;\n",
    "        border-radius: 12px;\n",
    "        padding: 20px;\n",
    "        text-align: center;\n",
    "        margin: 12px 0;\n",
    "        font-size: 32px;\n",
    "        font-weight: 700;\n",
    "        color: #64748b;\n",
    "        min-height: 85px;\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "        justify-content: center;\n",
    "        transition: all 0.3s ease;\n",
    "        font-family: 'Inter', monospace;\n",
    "    }\n",
    "    \n",
    "    .time-display.processing {\n",
    "        border-color: #f59e0b;\n",
    "        color: #d97706;\n",
    "        background: #fffbeb;\n",
    "        animation: pulse 2s infinite;\n",
    "    }\n",
    "    \n",
    "    .time-display.success {\n",
    "        border-color: #10b981;\n",
    "        color: #059669;\n",
    "        background: #f0fdf4;\n",
    "    }\n",
    "    \n",
    "    .time-display.error {\n",
    "        border-color: #ef4444;\n",
    "        color: #dc2626;\n",
    "        background: #fef2f2;\n",
    "    }\n",
    "    \n",
    "    @keyframes pulse {\n",
    "        0%, 100% { opacity: 1; }\n",
    "        50% { opacity: 0.7; }\n",
    "    }\n",
    "    \n",
    "    /* Example buttons */\n",
    "    .example-btn { \n",
    "        margin: 3px;\n",
    "        border-radius: 8px;\n",
    "        transition: all 0.2s ease;\n",
    "        background: #ffffff;\n",
    "        border: 1px solid #e2e8f0;\n",
    "        color: #475569;\n",
    "        font-size: 13px;\n",
    "        padding: 8px 12px;\n",
    "    }\n",
    "    \n",
    "    .example-btn:hover { \n",
    "        background: #f8fafc;\n",
    "        border-color: #3b82f6;\n",
    "        color: #1e40af;\n",
    "        transform: translateY(-1px);\n",
    "        box-shadow: 0 4px 12px rgba(59, 130, 246, 0.15);\n",
    "    }\n",
    "    \n",
    "    /* Tech panel */\n",
    "    .tech-panel {\n",
    "        background: #f8fafc;\n",
    "        border: 1px solid #e2e8f0;\n",
    "        border-radius: 12px;\n",
    "        padding: 20px;\n",
    "        margin: 20px 0;\n",
    "    }\n",
    "    \n",
    "    /* Performance panel */\n",
    "    .perf-panel {\n",
    "        background: #f0fdf4;\n",
    "        border: 1px solid #bbf7d0;\n",
    "        border-radius: 12px;\n",
    "        padding: 16px;\n",
    "        margin: 16px 0;\n",
    "    }\n",
    "    \n",
    "    /* Category headers */\n",
    "    .category-header {\n",
    "        color: #475569;\n",
    "        font-weight: 600;\n",
    "        margin: 20px 0 12px 0;\n",
    "        font-size: 15px;\n",
    "        padding-bottom: 6px;\n",
    "        border-bottom: 2px solid #e2e8f0;\n",
    "    }\n",
    "    \n",
    "    /* Footer */\n",
    "    .footer {\n",
    "        background: #f8fafc;\n",
    "        border-radius: 12px;\n",
    "        border: 1px solid #e2e8f0;\n",
    "        padding: 20px;\n",
    "        margin-top: 32px;\n",
    "        text-align: center;\n",
    "    }\n",
    "    \"\"\",\n",
    "    title=\"Chatbot Prawniczy KRUS\"\n",
    ") as demo:\n",
    "    \n",
    "    # Header\n",
    "    gr.HTML(\"\"\"\n",
    "    <div class=\"header\">\n",
    "        <h1 style=\"margin: 0; font-size: 2.4em; font-weight: 600; color: #1e40af;\">\n",
    "            ⚖️ Chatbot Prawniczy KRUS\n",
    "        </h1>\n",
    "        <p style=\"margin: 12px 0 0 0; font-size: 1.1em; color: #64748b; font-weight: 400;\">\n",
    "            System Analizy Prawnej\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        # Kolumna główna - Chat\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"💬 Konsultacja prawnicza\",\n",
    "                height=520,\n",
    "                bubble_full_width=False,\n",
    "                show_copy_button=True\n",
    "            )\n",
    "            \n",
    "            status_display = gr.HTML(\n",
    "                value='<div class=\"status-panel\">💭 System gotowy do analizy prawnej</div>'\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                user_input = gr.Textbox(\n",
    "                    label=\"Pytanie prawnicze\",\n",
    "                    placeholder=\"Np: Jak oblicza się wysokość emerytury rolniczej?\",\n",
    "                    lines=2,\n",
    "                    scale=4\n",
    "                )\n",
    "                send_btn = gr.Button(\"🚀 Analizuj\", variant=\"primary\", size=\"lg\", scale=1)\n",
    "            \n",
    "            with gr.Row():\n",
    "                reset_memory = gr.Checkbox(\n",
    "                    label=\"🔄 Reset kontekstu\", \n",
    "                    value=False,\n",
    "                    info=\"Wyczyść pamięć poprzednich pytań\"\n",
    "                )\n",
    "                clear_btn = gr.Button(\"🗑️ Nowa sesja\", variant=\"secondary\")\n",
    "        \n",
    "        # Kolumna boczna\n",
    "        with gr.Column(scale=2):\n",
    "            gr.HTML('<h3 style=\"margin: 20px 0 15px 0; color: #475569; font-weight: 600;\">⚡ Czas odpowiedzi</h3>')\n",
    "            \n",
    "            response_time_display = gr.HTML('<div class=\"time-display\">Oczekiwanie...</div>')\n",
    "            \n",
    "            # Przykładowe pytania\n",
    "            gr.HTML('<h3 style=\"margin: 32px 0 20px 0; color: #475569; font-weight: 600;\">💡 Przykłady</h3>')\n",
    "            \n",
    "            gr.HTML('<h4 class=\"category-header\">🏛️ Emerytury</h4>')\n",
    "            emerytura_examples = [\n",
    "                \"Warunki nabycia prawa do emerytury rolniczej\",\n",
    "                \"Wymagania wiekowe dla emerytury\", \n",
    "                \"Obliczanie wysokości emerytury\"\n",
    "            ]\n",
    "            \n",
    "            for example in emerytura_examples:\n",
    "                btn = gr.Button(f\"📋 {example}\", variant=\"secondary\", size=\"sm\", elem_classes=[\"example-btn\"])\n",
    "                btn.click(fn=lambda q=example: q, outputs=user_input)\n",
    "            \n",
    "            gr.HTML('<h4 class=\"category-header\">🛡️ Ubezpieczenia</h4>')\n",
    "            ubezpieczenia_examples = [\n",
    "                \"Kto podlega ubezpieczeniu w KRUS?\",\n",
    "                \"Składki na ubezpieczenie rolników\",\n",
    "                \"Definicja gospodarstwa rolnego\"\n",
    "            ]\n",
    "            \n",
    "            for example in ubezpieczenia_examples:\n",
    "                btn = gr.Button(f\"📋 {example}\", variant=\"secondary\", size=\"sm\", elem_classes=[\"example-btn\"])\n",
    "                btn.click(fn=lambda q=example: q, outputs=user_input)\n",
    "            \n",
    "            gr.HTML('<h4 class=\"category-header\">💰 Świadczenia</h4>')\n",
    "            swiadczenia_examples = [\n",
    "                \"Prawo do renty rodzinnej\",\n",
    "                \"Dokumenty przy wniosku o świadczenia\", \n",
    "                \"Okres wypłaty zasiłku macierzyńskiego\"\n",
    "            ]\n",
    "            \n",
    "            for example in swiadczenia_examples:\n",
    "                btn = gr.Button(f\"📋 {example}\", variant=\"secondary\", size=\"sm\", elem_classes=[\"example-btn\"])\n",
    "                btn.click(fn=lambda q=example: q, outputs=user_input)\n",
    "            \n",
    "            # Panel techniczny\n",
    "            gr.HTML(\"\"\"\n",
    "            <div class=\"tech-panel\">\n",
    "                <h3 style=\"margin: 0 0 12px 0; color: #374151; font-weight: 600; font-size: 16px;\">🔧 Optymalizacja</h3>\n",
    "                <div style=\"font-size: 12px; line-height: 1.6; color: #64748b;\">\n",
    "                    <strong>Tryb:</strong><br>\n",
    "                    <strong>Overhead:</strong> Zminimalizowany<br>\n",
    "                    <strong>Czas:</strong> ask() + ~1-2s Gradio<br>\n",
    "                    <strong>Queue:</strong> Wyłączona<br>\n",
    "                    <strong>Network:</strong> 1 request zamiast 100+\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\")\n",
    "            \n",
    "            # Wskaźniki\n",
    "            gr.HTML(\"\"\"\n",
    "            <div class=\"perf-panel\">\n",
    "                <h4 style=\"color: #065f46; margin: 0 0 8px 0; font-weight: 600; font-size: 14px;\">📈 Wydajność</h4>\n",
    "                <div style=\"font-size: 11px; color: #059669; line-height: 1.5;\">\n",
    "                    <strong>Czas:</strong> ~20-25s (vs 90s ze streamowaniem)<br>\n",
    "                    <strong>Requests:</strong> 1 (vs 100+)<br>\n",
    "                    <strong>CPU:</strong> Minimalne obciążenie\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\")\n",
    "    \n",
    "    # Funkcje obsługi - BEZ KOLEJKI I GENERATORÓW\n",
    "    def save_question_and_clear(msg, history):\n",
    "        global current_question\n",
    "        current_question = msg.strip()\n",
    "        print(f\"DEBUG: Zapisano pytanie: '{current_question}'\")\n",
    "        return history, \"\"\n",
    "\n",
    "    def clear_chat():\n",
    "        global current_question\n",
    "        current_question = \"\"\n",
    "        return [], \"💭 System zresetowany\", '<div class=\"time-display\">-</div>'\n",
    "    \n",
    "    # Event binding - WSZYSTKO BEZ QUEUE\n",
    "    send_btn.click(\n",
    "        fn=save_question_and_clear,\n",
    "        inputs=[user_input, chatbot],\n",
    "        outputs=[chatbot, user_input],\n",
    "        queue=False\n",
    "    ).then(\n",
    "        fn=fast_answer,  # Funkcja bez generatora!\n",
    "        inputs=[reset_memory, chatbot],\n",
    "        outputs=[chatbot, status_display, response_time_display],\n",
    "        queue=False  # Kluczowe - bez kolejki\n",
    "    )\n",
    "    \n",
    "    user_input.submit(\n",
    "        fn=save_question_and_clear,\n",
    "        inputs=[user_input, chatbot],\n",
    "        outputs=[chatbot, user_input],\n",
    "        queue=False\n",
    "    ).then(\n",
    "        fn=fast_answer,\n",
    "        inputs=[reset_memory, chatbot],\n",
    "        outputs=[chatbot, status_display, response_time_display],\n",
    "        queue=False\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(\n",
    "        fn=clear_chat,\n",
    "        outputs=[chatbot, status_display, response_time_display],\n",
    "        queue=False\n",
    "    )\n",
    "    \n",
    "    # Footer\n",
    "    gr.HTML(\"\"\"\n",
    "    <div class=\"footer\">\n",
    "        <p style=\"margin: 0 0 8px 0; font-weight: 600; color: #374151;\">\n",
    "            ⚖️ Chatbot Prawniczy KRUS v2.0 - Fast Mode\n",
    "        </p>\n",
    "        <p style=\"font-size: 11px; color: #64748b; margin: 0; line-height: 1.4;\">\n",
    "            Bez streamowania = prawdziwy czas odpowiedzi. System służy celom informacyjnym.\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n",
    "# Uruchomienie BEZ KOLEJKI\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"⚖️ CHATBOT PRAWNICZY KRUS - FAST MODE\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"📊 Czas = ask() + ~1-2s (zamiast +70s)\")\n",
    "    print(\"🚀 Uruchamiam na porcie 7882...\")\n",
    "    \n",
    "    # BEZ demo.queue() - to było główne źródło opóźnień\n",
    "    demo.launch(\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7890,\n",
    "        share=True,\n",
    "        show_error=True,\n",
    "        inbrowser=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
