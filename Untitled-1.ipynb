{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08452d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0.dev20250903+cu128 12.8\n",
      "CUDA available: True GPUs: 2\n",
      "4.78 ms ± 16.1 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "print(torch.__version__, torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available(), \"GPUs:\", torch.cuda.device_count())\n",
    "x = torch.randn(8192, 8192, device=\"cuda:0\", dtype=torch.bfloat16)\n",
    "y = torch.randn(8192, 8192, device=\"cuda:0\", dtype=torch.bfloat16)\n",
    "torch.cuda.synchronize()\n",
    "%timeit (x @ y); torch.cuda.synchronize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b257ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"KRUS-debug\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d52b95ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "165dc343",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorDEBUG = True\n",
    "DEBUG = sorDEBUG\n",
    "RETURN_STRING_WHEN_DEBUG_FALSE = True\n",
    "\n",
    "\n",
    "DATA_PATH = \"C:/Users/admin/Desktop/krus-chatbot/AgroBot/data/ustawa_with_paragraph_headers.md\"\n",
    "PERSIST_PATH = \"chroma_ustawa\"\n",
    "\n",
    "EMBEDDER_MODEL   = \"intfloat/multilingual-e5-small\"\n",
    "RERANKER_MODEL   = \"radlab/polish-cross-encoder\"\n",
    "CLASSIFIER_MODEL = \"klasyfikator\"  # (niewykorzystywany tutaj, ale zostawiam)\n",
    "BASE_MODEL_ID    = \"speakleash/Bielik-11B-v2.6-Instruct\"\n",
    "# alternatywnie: \"CYFRAGOVPL/PLLuM-12B-chat\"\n",
    "\n",
    "RERANK_THRESHOLD = 0.30\n",
    "K_SIM   = 10\n",
    "K_FINAL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd8dd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0.dev20250903+cu128 | CUDA available: True | GPUs: 2\n",
      "Platform: Windows-11-10.0.26100-SP0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:09<00:00,  1.90s/it]\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: oba modele załadowane bez kwantyzacji.\n"
     ]
    }
   ],
   "source": [
    "import os, torch, platform\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"   # stabilniej na Windowsie\n",
    "torch.backends.cuda.matmul.allow_tf32 = True     # RTX = szybciej\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available(), \"| GPUs:\", torch.cuda.device_count())\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "BASE_MODEL_ID = BASE_MODEL_ID        # ← zostaw swoją nazwę\n",
    "CLASSIFIER_MODEL = CLASSIFIER_MODEL  # ← zostaw swoją nazwę\n",
    "\n",
    "# 1) Tokenizery (fast → fallback na slow, jeśli trzeba)\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=True, trust_remote_code=False)\n",
    "except Exception as e:\n",
    "    print(\"Fast tokenizer fail → slow:\", e)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=False)\n",
    "\n",
    "try:\n",
    "    classifier_tok = AutoTokenizer.from_pretrained(CLASSIFIER_MODEL, use_fast=True, trust_remote_code=False)\n",
    "except Exception as e:\n",
    "    print(\"Fast classifier tokenizer fail → slow:\", e)\n",
    "    classifier_tok = AutoTokenizer.from_pretrained(CLASSIFIER_MODEL, use_fast=False)\n",
    "\n",
    "# 2) MODELE — BEZ bitsandbytes\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    dtype=torch.bfloat16,       # 5090 lubi bf16\n",
    "    device_map=\"auto\",                # rozkład po GPU\n",
    "    low_cpu_mem_usage=False,\n",
    "    attn_implementation=\"sdpa\",       # bez FA2 → stabilniej\n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "\n",
    "clf_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CLASSIFIER_MODEL,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "\n",
    "print(\"OK: oba modele załadowane bez kwantyzacji.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a4ec07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INDEX] Liczba dokumentów (ustępów): 444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11664\\2375576860.py:49: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceBgeEmbeddings(\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11664\\2375576860.py:62: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "import shutil, re\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "persist_path = PERSIST_PATH\n",
    "shutil.rmtree(persist_path, ignore_errors=True)\n",
    "os.makedirs(persist_path, exist_ok=True)\n",
    "\n",
    "docs_raw = TextLoader(DATA_PATH, encoding=\"utf-8\").load()\n",
    "header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"###\", \"ustep\")])\n",
    "chunks = header_splitter.split_text(docs_raw[0].page_content)\n",
    "\n",
    "# meta: <!-- chapter:1 article:16a paragraph:3 id:ch1-art16a-ust3 -->\n",
    "meta_re = re.compile(\n",
    "    r\"<!--\\s*chapter\\s*:\\s*(\\d+)\\s+article\\s*:\\s*([0-9a-z]+)\\s+paragraph\\s*:\\s*([0-9a-z]+)\\s+id\\s*:\\s*([^\\s>]+)\\s*-->\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "normed: List[Document] = []\n",
    "for d in chunks:\n",
    "    md = dict(d.metadata)\n",
    "    header_text = md.get(\"ustep\", \"\") or \"\"\n",
    "    source_for_meta = header_text + \"\\n\" + d.page_content\n",
    "    m = meta_re.search(source_for_meta)\n",
    "    if m:\n",
    "        md[\"chapter\"]   = int(m.group(1))\n",
    "        md[\"article\"]   = m.group(2).lower()\n",
    "        md[\"paragraph\"] = m.group(3).lower()\n",
    "        md[\"id\"]        = m.group(4)\n",
    "        md[\"rozdzial\"]  = md[\"chapter\"]\n",
    "        md[\"artykul\"]   = md[\"article\"]\n",
    "        md[\"ust\"]       = md[\"paragraph\"]\n",
    "\n",
    "    clean_header = meta_re.sub(\"\", header_text).strip()\n",
    "    if clean_header:\n",
    "        md[\"ustep\"] = clean_header\n",
    "\n",
    "    content = meta_re.sub(\"\", d.page_content).strip()\n",
    "    normed.append(Document(page_content=content, metadata=md))\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"[INDEX] Liczba dokumentów (ustępów): {len(normed)}\")\n",
    "\n",
    "embedder = HuggingFaceBgeEmbeddings(\n",
    "    model_name=EMBEDDER_MODEL,\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "shutil.rmtree(persist_path, ignore_errors=True)\n",
    "db = Chroma.from_documents(\n",
    "    documents=normed,\n",
    "    embedding=embedder,\n",
    "    persist_directory=persist_path,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "db.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b3291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11664\\3291730828.py:120: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n",
      "Device set to use cuda:0\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11664\\3291730828.py:138: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=hf_pipe)\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "K_SIM = globals().get(\"K_SIM\", 12)\n",
    "K_FINAL = globals().get(\"K_FINAL\", 6)\n",
    "RERANK_THRESHOLD = globals().get(\"RERANK_THRESHOLD\", None)  # None = bez progu\n",
    "RERANKER_MODEL = globals().get(\"RERANKER_MODEL\", \"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "import os, re, shutil, unicodedata, asyncio\n",
    "from typing import List, Optional, Dict, Any, Tuple, Callable\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# LangChain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.callbacks import CallbackManager\n",
    "from langchain.callbacks.tracers.langchain import LangChainTracer\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.documents import Document  # UŻYWAMY JEDNEGO typu Document!\n",
    "\n",
    "# Reranker + LLM\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import pipeline as hf_pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "# -----------------------------\n",
    "# 2) UTIL: normalizacja ogonków\n",
    "# -----------------------------\n",
    "def strip_accents_lower(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    return s.lower().strip()\n",
    "\n",
    "# -----------------------------\n",
    "# 3) (opcjonalnie) SPRAWDŹ ZASOBY\n",
    "# -----------------------------\n",
    "# Wymagamy:\n",
    "# - globalny `db` (Chroma) — z indeksem ustawy\n",
    "# - globalny `model`, `tokenizer` — Twój LLM (np. Bielik)\n",
    "if \"db\" not in globals():\n",
    "    raise RuntimeError(\"Brak globalnej bazy `db` (Chroma). Zainicjalizuj ją przed załadowaniem skryptu.\")\n",
    "\n",
    "model = gen_model\n",
    "\n",
    "if globals().get(\"model\", None) is None or globals().get(\"tokenizer\", None) is None:\n",
    "    raise RuntimeError(\"Załaduj wcześniej LLM do zmiennych `model` i `tokenizer`.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) RERANKER (CrossEncoder)\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cross_encoder = CrossEncoder(RERANKER_MODEL, device=device)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) PARSER: explicit refs (art./ust./pkt/lit)\n",
    "# -----------------------------\n",
    "REF_RE_EXT = re.compile(\n",
    "    r\"(?:art\\.?\\s*(?P<art>[0-9]+[a-z]?))\"\n",
    "    r\"(?:\\s*(?:ust(?:\\.|ęp)?|ustep)\\s*(?P<ust>[0-9]+[a-z]?))?\"\n",
    "    r\"(?:\\s*(?:pkt\\.?)\\s*(?P<pkt>[0-9]+[a-z]?))?\"\n",
    "    r\"(?:\\s*(?:lit\\.?)\\s*(?P<lit>[a-z]))?\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def parse_ref_ext(query: str) -> Optional[Dict[str, str]]:\n",
    "    m = REF_RE_EXT.search(query or \"\")\n",
    "    if not m:\n",
    "        return None\n",
    "    ref = {}\n",
    "    if m.group(\"art\"): ref[\"article\"]   = m.group(\"art\").lower()\n",
    "    if m.group(\"ust\"): ref[\"paragraph\"] = m.group(\"ust\").lower()\n",
    "    if m.group(\"pkt\"): ref[\"punkt\"]     = m.group(\"pkt\").lower()\n",
    "    if m.group(\"lit\"): ref[\"litera\"]    = m.group(\"lit\").lower()\n",
    "    return ref if ref else None\n",
    "\n",
    "# -----------------------------\n",
    "# 6) RETRIEVAL podstawowy + rerank\n",
    "# -----------------------------\n",
    "def retrieve_basic(query: str, k_sim: int = K_SIM, k_final: int = K_FINAL, rerank_threshold: float | None = RERANK_THRESHOLD) -> List[Document]:\n",
    "    ref = parse_ref_ext(query)\n",
    "    filter_dict = {}\n",
    "    if ref:\n",
    "        if \"article\" in ref:   filter_dict[\"article\"]   = ref[\"article\"]\n",
    "        if \"paragraph\" in ref: filter_dict[\"paragraph\"] = ref[\"paragraph\"]\n",
    "    if not filter_dict:\n",
    "        filter_dict = None\n",
    "\n",
    "    docs = db.similarity_search(query, k=k_sim, filter=filter_dict)\n",
    "    if not docs:\n",
    "        return []\n",
    "\n",
    "    pairs = [(query, d.page_content) for d in docs]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    scores = np.asarray(scores, dtype=float)\n",
    "\n",
    "    scored_docs = [(d, s) for d, s in zip(docs, scores)]\n",
    "    if rerank_threshold is not None:\n",
    "        scored_docs = [(d, s) for d, s in scored_docs if s >= rerank_threshold]\n",
    "        if not scored_docs:\n",
    "            return []\n",
    "\n",
    "    scored_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)[:k_final]\n",
    "    out: List[Document] = []\n",
    "    for d, s in scored_docs:\n",
    "        md = dict(d.metadata or {})\n",
    "        md[\"rerank_score\"] = float(s)\n",
    "        d.metadata = md\n",
    "        out.append(d)\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# 7) PAMIĘĆ / LLM\n",
    "# -----------------------------\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=3, memory_key=\"chat_history\", return_messages=True, output_key=\"answer\"\n",
    ")\n",
    "\n",
    "hf_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=globals().get(\"model\"),\n",
    "    tokenizer=globals().get(\"tokenizer\"),\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.35,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.0,\n",
    "    pad_token_id=(getattr(globals().get(\"tokenizer\"), \"eos_token_id\", None)),\n",
    "    eos_token_id=(getattr(globals().get(\"tokenizer\"), \"eos_token_id\", None)),\n",
    "    return_full_text=False\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipe)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"### System:\\n\"\n",
    "        \"Jesteś ekspertem prawa ubezpieczeń społecznych rolników. \"\n",
    "        \"Odpowiadasz WYŁĄCZNIE na podstawie Dokumentów poniżej. \"\n",
    "        \"Twoim zadaniem jest odpowiedzenie na pytanie najlepiej jak możesz wyłącznie na podstawie Dokumentów. \"\n",
    "        \"Nie zmyślaj informacji i jeśli w dokumentach brak odpowiedzi, poinformuj o tym.\\n\"\n",
    "        \"Formatowanie: nie używaj formatowania Markdown (**…**) ani __…__.\\n\"\n",
    "        \"Napisz maksymalnie 6-8 zdań lub 8 punktów.\"\n",
    "        \"### User:\\n{question}\\n\\n\"\n",
    "        \"### Dokumenty:\\n{context}\\n\"\n",
    "        \"### Asystent:\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "tracer = LangChainTracer()\n",
    "callback_manager = CallbackManager([tracer])\n",
    "\n",
    "class FunctionRetriever(BaseRetriever):\n",
    "    k_sim: int\n",
    "    k_final: int\n",
    "    rerank_threshold: Optional[float] = None\n",
    "    _fn: Callable[..., List[Document]] = PrivateAttr()\n",
    "\n",
    "    def __init__(self, fn: Callable[..., List[Document]], **data):\n",
    "        super().__init__(**data)\n",
    "        self._fn = fn\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self._fn(query, k_sim=self.k_sim, k_final=self.k_final, rerank_threshold=self.rerank_threshold)\n",
    "\n",
    "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(None, lambda: self._get_relevant_documents(query))\n",
    "\n",
    "init_retriever = FunctionRetriever(fn=retrieve_basic, k_sim=K_SIM, k_final=K_FINAL, rerank_threshold=RERANK_THRESHOLD)\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=init_retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,\n",
    "    output_key=\"answer\",\n",
    "    callback_manager=callback_manager\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 8) FORMATOWANIE / DEBUG + finalizer\n",
    "# -----------------------------\n",
    "def _build_citations_block(docs: List[Document]) -> str:\n",
    "    if not docs:\n",
    "        return \"Cytowane ustępy:\\n(brak)\\n\"\n",
    "    lines = [\"Cytowane ustępy:\"]\n",
    "    for d in docs:\n",
    "        md = d.metadata or {}\n",
    "        rozdz = md.get(\"rozdzial\", md.get(\"chapter\"))\n",
    "        art   = md.get(\"artykul\",  md.get(\"article\"))\n",
    "        ust   = md.get(\"ust\",      md.get(\"paragraph\"))\n",
    "        pid   = md.get(\"id\", f\"ch{rozdz}-art{art}-ust{ust}\")\n",
    "        lines.append(f\"- [{pid}] Rozdz.{rozdz} Art.{art} Ust.{ust}\")\n",
    "    return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "def format_docs_for_prompt(docs: List[Document]) -> str:\n",
    "    blocks = []\n",
    "    for d in docs:\n",
    "        md = d.metadata or {}\n",
    "        rozdz = md.get(\"rozdzial\", md.get(\"chapter\"))\n",
    "        art   = md.get(\"artykul\",  md.get(\"article\"))\n",
    "        ust   = md.get(\"ust\",      md.get(\"paragraph\"))\n",
    "        pid   = md.get(\"id\", f\"ch{rozdz}-art{art}-ust{ust}\")\n",
    "        blocks.append(f\"[{pid}]\\n{d.page_content}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "def log_rerank_scores(docs: List[Document], header: str = \"DEBUG rerank scores\") -> None:\n",
    "    if not DEBUG: return\n",
    "    print(header)\n",
    "    if not docs:\n",
    "        print(\"(brak dokumentów)\"); return\n",
    "    for d in docs:\n",
    "        md = d.metadata or {}\n",
    "        pid = md.get(\"id\") or f\"ch{md.get('chapter')}-art{md.get('article')}-ust{md.get('paragraph')}\"\n",
    "        sc  = md.get(\"rerank_score\")\n",
    "        print(f\"- {pid}: score={sc:.6f}\" if isinstance(sc, (int, float)) else f\"- {pid}: score=(brak)\")\n",
    "\n",
    "# --- czyszczenie markdown bold/underline ---\n",
    "def strip_markdown_bold(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    text = re.sub(r\"\\*\\*(.*?)\\*\\*\", r\"\\1\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"__(.*?)__\", r\"\\1\", text, flags=re.DOTALL)\n",
    "    # osierocone znaczniki\n",
    "    text = text.replace(\"**\", \"\").replace(\"__\", \"\")\n",
    "    return text\n",
    "\n",
    "# --- trimming niedomkniętej końcówki / pustych punktorów ---\n",
    "_SENT_ENDERS = \".?!…\"\n",
    "_CLOSERS = \"”»\\\")]’\"\n",
    "\n",
    "def _rstrip_u(s: str) -> str:\n",
    "    return s.rstrip(\" \\t\\r\\n\\u00A0\")\n",
    "\n",
    "_LIST_MARKER_ONLY_RE = re.compile(\n",
    "    r\"\"\"^[\\s\\u00A0]*(\n",
    "            [-*+•·—–]\n",
    "          | (?:\\(?\\d+[a-z]?\\)|\\d+[a-z]?[.)])\n",
    "          | (?:\\(?[ivxlcdm]+\\)|[ivxlcdm]+[.)])\n",
    "          | (?:[a-z][.)])\n",
    "        )[\\s\\u00A0]*$\"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE\n",
    ")\n",
    "\n",
    "def _strip_trailing_empty_list_item(s: str) -> tuple[str, bool]:\n",
    "    if not s:\n",
    "        return s, False\n",
    "    s2 = _rstrip_u(s)\n",
    "    if not s2:\n",
    "        return s2, (s2 != s)\n",
    "    lines = s2.splitlines()\n",
    "    last = _rstrip_u(lines[-1])\n",
    "    if _LIST_MARKER_ONLY_RE.match(last or \"\"):\n",
    "        return _rstrip_u(\"\\n\".join(lines[:-1])), True\n",
    "    return s2, False\n",
    "\n",
    "def _ends_with_full_stop(s: str) -> bool:\n",
    "    return re.search(rf\"[{re.escape(_SENT_ENDERS)}][{re.escape(_CLOSERS)}]*[\\s\\u00A0]*$\", s) is not None\n",
    "\n",
    "# pomocnicze do skrótów\n",
    "_ABBR_SET = {\"art\",\"ust\",\"pkt\",\"lit\",\"tj\",\"tzw\",\"np\",\"itd\",\"itp\",\"m.in\",\"prof\",\"dr\",\"nr\",\"poz\",\"cd\",\"al\",\"ul\",\"pl\",\"św\",\"sw\"}\n",
    "\n",
    "def _last_token_before_dot_for_trim(buf: str) -> str:\n",
    "    m = re.search(r\"([A-Za-zÀ-ÖØ-öø-ÿŁŚŻŹĆŃÓÄÖÜĄĘłśżźćńóäöüąę]+)\\.\\s*$\", buf)\n",
    "    return (m.group(1).lower() if m else \"\")\n",
    "\n",
    "def _is_abbreviation_dot(text: str, dot_pos: int) -> bool:\n",
    "    prev = text[:dot_pos+1]\n",
    "    tok = _last_token_before_dot_for_trim(prev)\n",
    "    return tok in _ABBR_SET\n",
    "\n",
    "def _find_last_safe_boundary(s: str) -> int | None:\n",
    "    i = len(s) - 1\n",
    "    while i >= 0 and (s[i].isspace() or s[i] in _CLOSERS or s[i] == \"\\u00A0\"):\n",
    "        i -= 1\n",
    "    while i >= 0:\n",
    "        ch = s[i]\n",
    "        if ch in _SENT_ENDERS:\n",
    "            if ch == \".\" and _is_abbreviation_dot(s[:i+1], i):\n",
    "                i -= 1\n",
    "                continue\n",
    "            return i + 1\n",
    "        i -= 1\n",
    "    return None\n",
    "\n",
    "def trim_incomplete_sentences(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    s = _rstrip_u(text)\n",
    "    changed = True\n",
    "    while changed:\n",
    "        s, changed = _strip_trailing_empty_list_item(s)\n",
    "    if s.endswith(\":\"):\n",
    "        last_nl = s.rfind(\"\\n\")\n",
    "        s = _rstrip_u(s[:last_nl]) if last_nl != -1 else \"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    if _ends_with_full_stop(s):\n",
    "        return s\n",
    "    cut = _find_last_safe_boundary(s)\n",
    "    if cut is None:\n",
    "        return s\n",
    "    return _rstrip_u(s[:cut])\n",
    "\n",
    "def _finalize_return(text: str, docs: List[Document], mode: str):\n",
    "    # zbuduj payload\n",
    "    debug = [{\"id\": (d.metadata or {}).get(\"id\"),\n",
    "              \"score\": (d.metadata or {}).get(\"rerank_score\")} for d in (docs or [])]\n",
    "    payload = {\"answer\": text, \"source_documents\": docs, \"debug\": {\"mode\": mode, \"rerank\": debug}}\n",
    "\n",
    "    if DEBUG:\n",
    "        print(\"\\nODPOWIEDŹ\\n\", text,\n",
    "              \"\\n\\n *Mogę popełniać błędy, skonsultuj się z placówką KRUS w celu potwierdzenia informacji.*\\n\")\n",
    "        return payload\n",
    "\n",
    "    if RETURN_STRING_WHEN_DEBUG_FALSE:\n",
    "        return text\n",
    "    return payload\n",
    "\n",
    "# -----------------------------\n",
    "# 9) SMALLTALK (krótko)\n",
    "# -----------------------------\n",
    "_SMALLTALK_RULES = [\n",
    "    (r\"^(czesc|cze|hej|heja|hejka|witam|siema|elo|halo|dzien dobry|dobry wieczor)\\b\",\n",
    "     \"Cześć! W czym mogę pomóc w sprawie KRUS/ustawy?\"),\n",
    "    (r\"\\b(dzieki|dziekuje|dzieki wielkie|dziekuje bardzo|thx|thanks)\\b\",\n",
    "     \"Nie ma sprawy! Jeśli chcesz, podaj kolejne pytanie.\"),\n",
    "]\n",
    "def smalltalk_reply(user_q: str) -> Optional[str]:\n",
    "    qn = strip_accents_lower(user_q)\n",
    "    for pat, resp in _SMALLTALK_RULES:\n",
    "        if re.search(pat, qn, flags=re.IGNORECASE):\n",
    "            return resp\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# 10) CTX-JUDGE (NLI TAK/NIE)\n",
    "# -----------------------------\n",
    "CTX_JUDGE_MODEL = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
    "ctx_judge = hf_pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=CTX_JUDGE_MODEL,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "# progi\n",
    "CTX_YES_MIN = 0.60\n",
    "CTX_NO_MIN  = 0.60\n",
    "CTX_MARGIN  = 0.12\n",
    "\n",
    "def judge_contextual(prev_q: str, curr_q: str) -> Tuple[Optional[bool], float, Dict[str, float]]:\n",
    "    if not prev_q or not curr_q:\n",
    "        return None, 0.0, {}\n",
    "    seq = (\n",
    "        f\"Pytanie A: {prev_q}\\n\"\n",
    "        f\"Pytanie B: {curr_q}\\n\"\n",
    "        f\"Czy Pytanie B jest dopytaniem/uzupełnieniem do Pytania A?\"\n",
    "    )\n",
    "    out = ctx_judge(\n",
    "        sequences=seq,\n",
    "        candidate_labels=[\"TAK\", \"NIE\"],\n",
    "        hypothesis_template=\"To jest {}.\"\n",
    "    )\n",
    "    dist = {lbl.upper(): float(score) for lbl, score in zip(out[\"labels\"], out[\"scores\"])}\n",
    "    yes, no = dist.get(\"TAK\", 0.0), dist.get(\"NIE\", 0.0)\n",
    "    margin = abs(yes - no)\n",
    "    if DEBUG:\n",
    "        print(f\"[CTX-JUDGE] dist={dist} margin={margin:.2f}\")\n",
    "    if yes >= CTX_YES_MIN and margin >= CTX_MARGIN:\n",
    "        return True, yes, dist\n",
    "    if no >= CTX_NO_MIN and margin >= CTX_MARGIN:\n",
    "        return False, no, dist\n",
    "    return None, max(yes, no), dist\n",
    "\n",
    "# -----------------------------\n",
    "# 11) STAN ROZMOWY\n",
    "# -----------------------------\n",
    "class ConversationState:\n",
    "    def __init__(self):\n",
    "        self.last_article_num: Optional[str] = None\n",
    "        self.last_paragraph_num: Optional[str] = None\n",
    "        self.last_docs: List[Document] = []\n",
    "        self.last_query: Optional[str] = None\n",
    "\n",
    "STATE = ConversationState()\n",
    "\n",
    "def _short_doc_label(d: Document) -> str:\n",
    "    md = d.metadata or {}\n",
    "    rozdz = md.get(\"rozdzial\", md.get(\"chapter\"))\n",
    "    art   = md.get(\"artykul\",  md.get(\"article\"))\n",
    "    ust   = md.get(\"ust\",      md.get(\"paragraph\"))\n",
    "    pid   = md.get(\"id\", f\"ch{rozdz}-art{art}-ust{ust}\")\n",
    "    return f\"{pid}\"\n",
    "\n",
    "def _update_state_from_docs(docs: List[Document], user_q: str):\n",
    "    if not docs: return\n",
    "    STATE.last_docs = docs[:]\n",
    "    STATE.last_query = user_q\n",
    "    md0 = docs[0].metadata or {}\n",
    "    STATE.last_article_num   = (md0.get(\"artykul\")  or md0.get(\"article\"))\n",
    "    STATE.last_paragraph_num = (md0.get(\"ust\")      or md0.get(\"paragraph\"))\n",
    "    if DEBUG:\n",
    "        print(f\"[STATE] last_article={STATE.last_article_num} last_paragraph={STATE.last_paragraph_num}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 12) REWRITER (dla follow_up)\n",
    "# -----------------------------\n",
    "def rewrite_query(user_q: str) -> str:\n",
    "    base = (STATE.last_query or \"\").strip()\n",
    "    if not base: return user_q\n",
    "    doc_labels = [_short_doc_label(d) for d in (STATE.last_docs or [])][:6]\n",
    "    doc_part = f\" (odnieś do: {', '.join(doc_labels)})\" if doc_labels else \"\"\n",
    "    return f\"{user_q} — w kontekście poprzedniego pytania: '{base}'{doc_part}\"\n",
    "\n",
    "# -----------------------------\n",
    "# 13) ROUTER (explicit refs / general)\n",
    "# -----------------------------\n",
    "def route_query(query: str):\n",
    "    qn = strip_accents_lower(query)\n",
    "    ref = parse_ref_ext(query)\n",
    "    if DEBUG:\n",
    "        print(f\"[ROUTER] raw='{query}' | norm='{qn}' | ref={ref}\")\n",
    "    if ref:\n",
    "        return \"EXPLICIT_REF\", {\"ref\": ref}\n",
    "    return \"GENERAL\", {\"query\": query}\n",
    "\n",
    "# -----------------------------\n",
    "# 14) Odpowiedź wyłącznie z podanych dokumentów\n",
    "# -----------------------------\n",
    "def _llm_generate(prompt_tmpl: PromptTemplate, **kwargs) -> str:\n",
    "    text = prompt_tmpl.format(**kwargs)\n",
    "    out = llm.invoke(text)\n",
    "    return (out or \"\").strip()\n",
    "\n",
    "def answer_from_docs(question: str, docs: List[Document]):\n",
    "    ctx = format_docs_for_prompt(docs) if docs else \"(brak dokumentów)\"\n",
    "    answer = _llm_generate(prompt, question=question, context=ctx)\n",
    "    answer = trim_incomplete_sentences(strip_markdown_bold(answer)) or answer\n",
    "    final_text = f\"{_build_citations_block(docs)}\\nOdpowiedź:\\n{answer}\"\n",
    "    return _finalize_return(final_text, docs, mode=\"restricted\")\n",
    "\n",
    "# -----------------------------\n",
    "# 15) GŁÓWNA FUNKCJA ask()\n",
    "# -----------------------------\n",
    "def ask(q: str, reset_memory: bool=False):\n",
    "    if reset_memory:\n",
    "        qa_chain.memory.clear()\n",
    "\n",
    "    # Smalltalk — bez RAG\n",
    "    st = smalltalk_reply(q)\n",
    "    if st is not None:\n",
    "        return _finalize_return(st, [], mode=\"smalltalk\")\n",
    "\n",
    "    # Router: explicit art./ust.?\n",
    "    action, payload = route_query(q)\n",
    "    if action == \"EXPLICIT_REF\":\n",
    "        ref = payload[\"ref\"]\n",
    "        flt = {}\n",
    "        if \"article\" in ref:   flt[\"article\"]   = ref[\"article\"]\n",
    "        if \"paragraph\" in ref: flt[\"paragraph\"] = ref[\"paragraph\"]\n",
    "        docs = db.similarity_search(\"treść przepisu\", k=5, filter=flt)\n",
    "        if DEBUG:\n",
    "            print(\"[ROUTER] EXPLICIT_REF → filter\", flt, \"→ docs:\", [ (d.metadata or {}).get(\"id\") for d in docs ])\n",
    "        if docs:\n",
    "            _update_state_from_docs(docs, q)\n",
    "            content = strip_markdown_bold(docs[0].page_content or \"\")\n",
    "            content = trim_incomplete_sentences(content) or content\n",
    "            final_text = f\"{_build_citations_block(docs)}\\nOdpowiedź (pełny przepis):\\n{content}\"\n",
    "            return _finalize_return(final_text, docs, mode=\"explicit\")\n",
    "        else:\n",
    "            return _finalize_return(\"Nie znalazłem takiego artykułu/ustępu.\", [], mode=\"explicit\")\n",
    "\n",
    "    # Jedyny router: sędzia NLI (TAK/NIE)\n",
    "    verdict, conf, dist = judge_contextual(STATE.last_query or \"\", q)\n",
    "    if DEBUG:\n",
    "        print(f\"[INTENT] judge_verdict={verdict} conf={conf:.2f}\")\n",
    "\n",
    "    mode = \"follow_up\" if verdict is True else \"new_query\"  # None traktujemy jak NIE\n",
    "\n",
    "    # FOLLOW_UP (z rewriterem, najpierw wąsko do ostatniego artykułu)\n",
    "    if mode == \"follow_up\":\n",
    "        rewritten = rewrite_query(q)\n",
    "        if DEBUG: print(f\"[REWRITE:follow_up] {rewritten}\")\n",
    "\n",
    "        docs_narrow: List[Document] = []\n",
    "        if STATE.last_article_num:\n",
    "            flt = {\"article\": STATE.last_article_num}\n",
    "            docs_tmp = db.similarity_search(rewritten, k=K_SIM, filter=flt)\n",
    "            if docs_tmp:\n",
    "                pairs = [(rewritten, d.page_content) for d in docs_tmp]\n",
    "                scores = cross_encoder.predict(pairs)\n",
    "                scored = sorted([(d, float(s)) for d, s in zip(docs_tmp, np.asarray(scores))],\n",
    "                                key=lambda x: x[1], reverse=True)[:K_FINAL]\n",
    "                for d, s in scored:\n",
    "                    md = dict(d.metadata or {}); md[\"rerank_score\"] = s; d.metadata = md\n",
    "                    docs_narrow.append(d)\n",
    "\n",
    "        if not docs_narrow and STATE.last_docs:\n",
    "            STATE.last_query = q\n",
    "            return answer_from_docs(rewritten, STATE.last_docs)\n",
    "\n",
    "        if not docs_narrow:\n",
    "            res = qa_chain.invoke({\"question\": rewritten})\n",
    "            out_docs = res.get(\"source_documents\", []) or []\n",
    "            _update_state_from_docs(out_docs, q)\n",
    "            log_rerank_scores(out_docs)\n",
    "            citations_block = _build_citations_block(out_docs)\n",
    "            raw_answer = (res.get(\"answer\") or \"\").strip()\n",
    "            raw_answer = trim_incomplete_sentences(strip_markdown_bold(raw_answer)) or raw_answer\n",
    "            final_text = f\"{citations_block}\\nOdpowiedź:\\n{raw_answer}\"\n",
    "            return _finalize_return(final_text, out_docs, mode=\"follow_up→global\")\n",
    "\n",
    "        _update_state_from_docs(docs_narrow, q)\n",
    "        return answer_from_docs(rewritten, docs_narrow)\n",
    "\n",
    "    # NEW_QUERY — globalny RAG\n",
    "    res = qa_chain.invoke({\"question\": q})\n",
    "    docs = res.get(\"source_documents\", []) or []\n",
    "    _update_state_from_docs(docs, q)\n",
    "    log_rerank_scores(docs)\n",
    "\n",
    "    citations_block = _build_citations_block(docs)\n",
    "    raw_answer = (res.get(\"answer\") or \"\").strip()\n",
    "    raw_answer = trim_incomplete_sentences(strip_markdown_bold(raw_answer)) or raw_answer\n",
    "    final_text = f\"{citations_block}\\nOdpowiedź:\\n{raw_answer}\"\n",
    "    return _finalize_return(final_text, docs, mode=\"new_query\")\n",
    "\n",
    "# ============================================================\n",
    "# KONIEC\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daf3476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ROUTER] raw='przysługuje wtedy jakaś renta??' | norm='przysługuje wtedy jakas renta??' | ref=None\n",
      "[CTX-JUDGE] dist={'TAK': 0.5873446464538574, 'NIE': 0.4126552939414978} margin=0.17\n",
      "[INTENT] judge_verdict=None conf=0.59\n",
      "[STATE] last_article=106 last_paragraph=2\n",
      "DEBUG rerank scores\n",
      "- ch9-art106-ust2: score=0.699651\n",
      "- ch2-art22-ust2: score=0.641719\n",
      "- ch2-art21-ust1: score=0.614756\n",
      "\n",
      "ODPOWIEDŹ\n",
      " Cytowane ustępy:\n",
      "- [ch9-art106-ust2] Rozdz.9 Art.106 Ust.2\n",
      "- [ch2-art22-ust2] Rozdz.2 Art.22 Ust.2\n",
      "- [ch2-art21-ust1] Rozdz.2 Art.21 Ust.1\n",
      "\n",
      "Odpowiedź:\n",
      "W przypadku niezdolności do samodzielnej egzystencji, która w kontekście ubezpieczeń społecznych rolników oznacza całkowitą niezdolność do pracy w gospodarstwie rolnym, przysługuje renta inwalidzka rolnicza. Jej wysokość i zasady przyznawania są określone w art. 22 i art. 24–28 ustawy o ubezpieczeniu społecznym rolników. Prawo do renty może być przywrócone, jeśli w ciągu 18 miesięcy od ustania poprzedniego prawa do renty, ubezpieczony ponownie stanie się całkowicie niezdolny do pracy w gospodarstwie rolnym. Renta ta przysługuje ubezpieczonemu, który spełnia warunki dotyczące okresu ubezpieczenia, całkowitej niezdolności do pracy oraz czasu powstania niezdolności. \n",
      "\n",
      " *Mogę popełniać błędy, skonsultuj się z placówką KRUS w celu potwierdzenia informacji.*\n",
      "\n",
      "{'answer': 'Cytowane ustępy:\\n- [ch9-art106-ust2] Rozdz.9 Art.106 Ust.2\\n- [ch2-art22-ust2] Rozdz.2 Art.22 Ust.2\\n- [ch2-art21-ust1] Rozdz.2 Art.21 Ust.1\\n\\nOdpowiedź:\\nW przypadku niezdolności do samodzielnej egzystencji, która w kontekście ubezpieczeń społecznych rolników oznacza całkowitą niezdolność do pracy w gospodarstwie rolnym, przysługuje renta inwalidzka rolnicza. Jej wysokość i zasady przyznawania są określone w art. 22 i art. 24–28 ustawy o ubezpieczeniu społecznym rolników. Prawo do renty może być przywrócone, jeśli w ciągu 18 miesięcy od ustania poprzedniego prawa do renty, ubezpieczony ponownie stanie się całkowicie niezdolny do pracy w gospodarstwie rolnym. Renta ta przysługuje ubezpieczonemu, który spełnia warunki dotyczące okresu ubezpieczenia, całkowitej niezdolności do pracy oraz czasu powstania niezdolności.', 'source_documents': [Document(metadata={'ust': '2', 'chapter': 9, 'rozdzial': 9, 'id': 'ch9-art106-ust2', 'ustep': 'Ust. 2', 'article': '106', 'artykul': '106', 'paragraph': '2', 'rerank_score': 0.6996508240699768}, page_content='Na wniosek osoby uprawnionej do okresowej renty inwalidzkiej, o której mowa w ust. 1, zamiast tej renty przyznaje\\nsię rentę inwalidzką rolniczą, przysługującą na zasadach i w wysokości określonych w art. 22 i art. 24–28.'), Document(metadata={'article': '22', 'ust': '2', 'artykul': '22', 'ustep': 'Ust. 2', 'chapter': 2, 'id': 'ch2-art22-ust2', 'rozdzial': 2, 'paragraph': '2', 'rerank_score': 0.6417189836502075}, page_content='Prawo do renty rolniczej z tytułu niezdolności do pracy, które ustało z powodu ustąpienia całkowitej niezdolności\\ndo pracy w gospodarstwie rolnym, podlega przywróceniu, jeżeli w ciągu 18 miesięcy od dnia ustania prawa do renty ubez\\npieczony ponownie stał się całkowicie niezdolny do pracy w gospodarstwie rolnym.'), Document(metadata={'id': 'ch2-art21-ust1', 'article': '21', 'ust': '1', 'rozdzial': 2, 'chapter': 2, 'paragraph': '1', 'artykul': '21', 'ustep': 'Ust. 1', 'rerank_score': 0.6147564649581909}, page_content='Renta rolnicza z tytułu niezdolności do pracy przysługuje ubezpieczonemu, który łącznie spełnia następu\\njące warunki:\\n1) podlegał ubezpieczeniu emerytalno-rentowemu przez wymagany okres, o którym mowa w ust. 2;\\n2) jest trwale lub okresowo całkowicie niezdolny do pracy w gospodarstwie rolnym;\\n3) całkowita niezdolność do pracy w gospodarstwie rolnym powstała w okresie podlegania ubezpieczeniu emerytalno--rentowemu lub w okresach, o których mowa w art. 20 ust. 1 pkt 1 i 2, lub nie później niż w ciągu 18 miesięcy od\\nustania tych okresów.')], 'debug': {'mode': 'new_query', 'rerank': [{'id': 'ch9-art106-ust2', 'score': 0.6996508240699768}, {'id': 'ch2-art22-ust2', 'score': 0.6417189836502075}, {'id': 'ch2-art21-ust1', 'score': 0.6147564649581909}]}}\n"
     ]
    }
   ],
   "source": [
    "print(ask(\"przysługuje wtedy jakaś renta??\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db22d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa040da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU count: 2\n",
      "0 NVIDIA GeForce RTX 5090\n",
      "1 NVIDIA GeForce RTX 5090\n",
      "GPU0: allocated=11.43 GB, reserved=11.58 GB\n",
      "GPU1: allocated=11.18 GB, reserved=11.20 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))\n",
    "\n",
    "model = gen_model\n",
    "# krótki „dry run”, żeby załadować cache\n",
    "prompt = tokenizer(\"Powiedz 'ok'.\", return_tensors=\"pt\")\n",
    "out = model.generate(**{k:v.to(model.device) for k,v in prompt.items()}, max_new_tokens=4)\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    alloc = torch.cuda.memory_allocated(i)/1e9\n",
    "    res   = torch.cuda.memory_reserved(i)/1e9\n",
    "    print(f\"GPU{i}: allocated={alloc:.2f} GB, reserved={res:.2f} GB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
